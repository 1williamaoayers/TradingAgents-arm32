#!/usr/bin/env python3
"""
å®æ—¶æ–°é—»æ•°æ®è·å–å·¥å…·
è§£å†³æ–°é—»æ»åæ€§é—®é¢˜
"""

import requests
import json
from datetime import datetime, timedelta
from zoneinfo import ZoneInfo

from typing import List, Dict, Optional
import time
import os
from dataclasses import dataclass

# å¯¼å…¥æ—¥å¿—æ¨¡å—
from tradingagents.config.runtime_settings import get_timezone_name

from tradingagents.utils.logging_manager import get_logger
logger = get_logger('agents')



@dataclass
class NewsItem:
    """æ–°é—»é¡¹ç›®æ•°æ®ç»“æ„"""
    title: str
    content: str
    source: str
    publish_time: datetime
    url: str
    urgency: str  # high, medium, low
    relevance_score: float


class RealtimeNewsAggregator:
    """å®æ—¶æ–°é—»èšåˆå™¨"""

    def __init__(self):
        self.headers = {
            'User-Agent': 'TradingAgents-CN/1.0'
        }

        # APIå¯†é’¥é…ç½®
        self.finnhub_key = os.getenv('FINNHUB_API_KEY')
        self.alpha_vantage_key = os.getenv('ALPHA_VANTAGE_API_KEY')
        self.newsapi_key = os.getenv('NEWSAPI_KEY')
        
        # ä¸­æ–‡è´¢ç»åª’ä½“æ–°é—»ç¼“å­˜
        # é‡è¦: è®¾ç½®15åˆ†é’Ÿç¼“å­˜,é˜²æ­¢é¢‘ç¹è¯·æ±‚è¢«é‡‘åæ•°æ®æ‹‰é»‘IP
        self._media_news_cache = None
        self._media_news_cache_time = None
        self._media_cache_duration = timedelta(minutes=15)  # 15åˆ†é’Ÿç¼“å­˜

    def get_realtime_stock_news(self, ticker: str, hours_back: int = 6, max_news: int = 10) -> List[NewsItem]:
        """
        è·å–å®æ—¶è‚¡ç¥¨æ–°é—»
        ä¼˜å…ˆçº§ï¼šä¸“ä¸šAPI > æ–°é—»API > æœç´¢å¼•æ“

        Args:
            ticker: è‚¡ç¥¨ä»£ç 
            hours_back: å›æº¯å°æ—¶æ•°
            max_news: æœ€å¤§æ–°é—»æ•°é‡ï¼Œé»˜è®¤10æ¡
        """
        logger.info(f"[æ–°é—»èšåˆå™¨] å¼€å§‹è·å– {ticker} çš„å®æ—¶æ–°é—»ï¼Œå›æº¯æ—¶é—´: {hours_back}å°æ—¶")
        start_time = datetime.now(ZoneInfo(get_timezone_name()))
        all_news = []

        # 1. FinnHubå®æ—¶æ–°é—» (æœ€é«˜ä¼˜å…ˆçº§)
        logger.info(f"[æ–°é—»èšåˆå™¨] å°è¯•ä» FinnHub è·å– {ticker} çš„æ–°é—»")
        finnhub_start = datetime.now(ZoneInfo(get_timezone_name()))
        finnhub_news = self._get_finnhub_realtime_news(ticker, hours_back)
        finnhub_time = (datetime.now(ZoneInfo(get_timezone_name())) - finnhub_start).total_seconds()

        if finnhub_news:
            logger.info(f"[æ–°é—»èšåˆå™¨] æˆåŠŸä» FinnHub è·å– {len(finnhub_news)} æ¡æ–°é—»ï¼Œè€—æ—¶: {finnhub_time:.2f}ç§’")
        else:
            logger.info(f"[æ–°é—»èšåˆå™¨] FinnHub æœªè¿”å›æ–°é—»ï¼Œè€—æ—¶: {finnhub_time:.2f}ç§’")

        all_news.extend(finnhub_news)

        # 2. Alpha Vantageæ–°é—»
        logger.info(f"[æ–°é—»èšåˆå™¨] å°è¯•ä» Alpha Vantage è·å– {ticker} çš„æ–°é—»")
        av_start = datetime.now(ZoneInfo(get_timezone_name()))
        av_news = self._get_alpha_vantage_news(ticker, hours_back)
        av_time = (datetime.now(ZoneInfo(get_timezone_name())) - av_start).total_seconds()

        if av_news:
            logger.info(f"[æ–°é—»èšåˆå™¨] æˆåŠŸä» Alpha Vantage è·å– {len(av_news)} æ¡æ–°é—»ï¼Œè€—æ—¶: {av_time:.2f}ç§’")
        else:
            logger.info(f"[æ–°é—»èšåˆå™¨] Alpha Vantage æœªè¿”å›æ–°é—»ï¼Œè€—æ—¶: {av_time:.2f}ç§’")

        all_news.extend(av_news)

        # 3. NewsAPI (å¦‚æœé…ç½®äº†)
        if self.newsapi_key:
            logger.info(f"[æ–°é—»èšåˆå™¨] å°è¯•ä» NewsAPI è·å– {ticker} çš„æ–°é—»")
            newsapi_start = datetime.now(ZoneInfo(get_timezone_name()))
            newsapi_news = self._get_newsapi_news(ticker, hours_back)
            newsapi_time = (datetime.now(ZoneInfo(get_timezone_name())) - newsapi_start).total_seconds()

            if newsapi_news:
                logger.info(f"[æ–°é—»èšåˆå™¨] æˆåŠŸä» NewsAPI è·å– {len(newsapi_news)} æ¡æ–°é—»ï¼Œè€—æ—¶: {newsapi_time:.2f}ç§’")
            else:
                logger.info(f"[æ–°é—»èšåˆå™¨] NewsAPI æœªè¿”å›æ–°é—»ï¼Œè€—æ—¶: {newsapi_time:.2f}ç§’")

            all_news.extend(newsapi_news)
        else:
            logger.info(f"[æ–°é—»èšåˆå™¨] NewsAPI å¯†é’¥æœªé…ç½®ï¼Œè·³è¿‡æ­¤æ–°é—»æº")

        # 4. ä¸­æ–‡è´¢ç»æ–°é—»æº
        logger.info(f"[æ–°é—»èšåˆå™¨] å°è¯•è·å– {ticker} çš„ä¸­æ–‡è´¢ç»æ–°é—»")
        chinese_start = datetime.now(ZoneInfo(get_timezone_name()))
        chinese_news = self._get_chinese_finance_news(ticker, hours_back)
        chinese_time = (datetime.now(ZoneInfo(get_timezone_name())) - chinese_start).total_seconds()

        if chinese_news:
            logger.info(f"[æ–°é—»èšåˆå™¨] æˆåŠŸè·å– {len(chinese_news)} æ¡ä¸­æ–‡è´¢ç»æ–°é—»ï¼Œè€—æ—¶: {chinese_time:.2f}ç§’")
        else:
            logger.info(f"[æ–°é—»èšåˆå™¨] æœªè·å–åˆ°ä¸­æ–‡è´¢ç»æ–°é—»ï¼Œè€—æ—¶: {chinese_time:.2f}ç§’")

        all_news.extend(chinese_news)

        # 5. Yahoo Finance æ–°é—» (æ¸¯è‚¡ä¼˜å…ˆ)
        # æ£€æµ‹æ˜¯å¦ä¸ºæ¸¯è‚¡ä»£ç 
        is_hk_stock = '.HK' in ticker.upper() or ticker.isdigit() and len(ticker) == 4
        
        if is_hk_stock:
            logger.info(f"[æ–°é—»èšåˆå™¨] æ£€æµ‹åˆ°æ¸¯è‚¡ä»£ç  {ticker}ï¼Œå°è¯•ä» Yahoo Finance è·å–æ–°é—»")
            yahoo_start = datetime.now(ZoneInfo(get_timezone_name()))
            yahoo_news = self._get_yahoo_finance_news(ticker, hours_back)
            yahoo_time = (datetime.now(ZoneInfo(get_timezone_name())) - yahoo_start).total_seconds()
            
            if yahoo_news:
                logger.info(f"[æ–°é—»èšåˆå™¨] æˆåŠŸä» Yahoo Finance è·å– {len(yahoo_news)} æ¡æ–°é—»ï¼Œè€—æ—¶: {yahoo_time:.2f}ç§’")
            else:
                logger.info(f"[æ–°é—»èšåˆå™¨] Yahoo Finance æœªè¿”å›æ–°é—»ï¼Œè€—æ—¶: {yahoo_time:.2f}ç§’")
            
            all_news.extend(yahoo_news)

        # 6. ä¸­æ–‡è´¢ç»åª’ä½“æ–°é—» (é‡‘åæ•°æ®ã€åå°”è¡—è§é—»ã€æ ¼éš†æ±‡)
        # é€šç”¨è´¢ç»å¿«è®¯,é€‚ç”¨äºæ‰€æœ‰å¸‚åœº
        logger.info(f"[æ–°é—»èšåˆå™¨] å°è¯•è·å–ä¸­æ–‡è´¢ç»åª’ä½“å¿«è®¯")
        media_start = datetime.now(ZoneInfo(get_timezone_name()))
        media_news = self._get_chinese_financial_media_news(hours_back)
        media_time = (datetime.now(ZoneInfo(get_timezone_name())) - media_start).total_seconds()
        
        if media_news:
            logger.info(f"[æ–°é—»èšåˆå™¨] æˆåŠŸä»ä¸­æ–‡è´¢ç»åª’ä½“è·å– {len(media_news)} æ¡æ–°é—»ï¼Œè€—æ—¶: {media_time:.2f}ç§’")
        else:
            logger.info(f"[æ–°é—»èšåˆå™¨] ä¸­æ–‡è´¢ç»åª’ä½“æœªè¿”å›æ–°é—»ï¼Œè€—æ—¶: {media_time:.2f}ç§’")
        
        all_news.extend(media_news)

        # å»é‡å’Œæ’åº
        logger.info(f"[æ–°é—»èšåˆå™¨] å¼€å§‹å¯¹ {len(all_news)} æ¡æ–°é—»è¿›è¡Œå»é‡å’Œæ’åº")
        dedup_start = datetime.now(ZoneInfo(get_timezone_name()))
        unique_news = self._deduplicate_news(all_news)
        sorted_news = sorted(unique_news, key=lambda x: x.publish_time, reverse=True)
        dedup_time = (datetime.now(ZoneInfo(get_timezone_name())) - dedup_start).total_seconds()

        # è®°å½•å»é‡ç»“æœ
        removed_count = len(all_news) - len(unique_news)
        logger.info(f"[æ–°é—»èšåˆå™¨] æ–°é—»å»é‡å®Œæˆï¼Œç§»é™¤äº† {removed_count} æ¡é‡å¤æ–°é—»ï¼Œå‰©ä½™ {len(sorted_news)} æ¡ï¼Œè€—æ—¶: {dedup_time:.2f}ç§’")

        # è®°å½•æ€»ä½“æƒ…å†µ
        total_time = (datetime.now(ZoneInfo(get_timezone_name())) - start_time).total_seconds()
        logger.info(f"[æ–°é—»èšåˆå™¨] {ticker} çš„æ–°é—»èšåˆå®Œæˆï¼Œæ€»å…±è·å– {len(sorted_news)} æ¡æ–°é—»ï¼Œæ€»è€—æ—¶: {total_time:.2f}ç§’")

        # é™åˆ¶æ–°é—»æ•°é‡ä¸ºæœ€æ–°çš„max_newsæ¡
        if len(sorted_news) > max_news:
            original_count = len(sorted_news)
            sorted_news = sorted_news[:max_news]
            logger.info(f"[æ–°é—»èšåˆå™¨] ğŸ“° æ–°é—»æ•°é‡é™åˆ¶: ä»{original_count}æ¡é™åˆ¶ä¸º{max_news}æ¡æœ€æ–°æ–°é—»")

        # è®°å½•ä¸€äº›æ–°é—»æ ‡é¢˜ç¤ºä¾‹
        if sorted_news:
            sample_titles = [item.title for item in sorted_news[:3]]
            logger.info(f"[æ–°é—»èšåˆå™¨] æ–°é—»æ ‡é¢˜ç¤ºä¾‹: {', '.join(sample_titles)}")

        return sorted_news

    def _get_finnhub_realtime_news(self, ticker: str, hours_back: int) -> List[NewsItem]:
        """è·å–FinnHubå®æ—¶æ–°é—»"""
        if not self.finnhub_key:
            return []

        try:
            # è®¡ç®—æ—¶é—´èŒƒå›´
            end_time = datetime.now(ZoneInfo(get_timezone_name()))
            start_time = end_time - timedelta(hours=hours_back)

            # FinnHub APIè°ƒç”¨
            url = "https://finnhub.io/api/v1/company-news"
            params = {
                'symbol': ticker,
                'from': start_time.strftime('%Y-%m-%d'),
                'to': end_time.strftime('%Y-%m-%d'),
                'token': self.finnhub_key
            }

            response = requests.get(url, params=params, headers=self.headers)
            response.raise_for_status()

            news_data = response.json()
            news_items = []

            for item in news_data:
                # æ£€æŸ¥æ–°é—»æ—¶æ•ˆæ€§
                publish_time = datetime.fromtimestamp(item.get('datetime', 0), tz=ZoneInfo(get_timezone_name()))
                if publish_time < start_time:
                    continue

                # è¯„ä¼°ç´§æ€¥ç¨‹åº¦
                urgency = self._assess_news_urgency(item.get('headline', ''), item.get('summary', ''))

                news_items.append(NewsItem(
                    title=item.get('headline', ''),
                    content=item.get('summary', ''),
                    source=item.get('source', 'FinnHub'),
                    publish_time=publish_time,
                    url=item.get('url', ''),
                    urgency=urgency,
                    relevance_score=self._calculate_relevance(item.get('headline', ''), ticker)
                ))

            return news_items

        except Exception as e:
            logger.error(f"FinnHubæ–°é—»è·å–å¤±è´¥: {e}")
            return []

    def _get_alpha_vantage_news(self, ticker: str, hours_back: int) -> List[NewsItem]:
        """è·å–Alpha Vantageæ–°é—»"""
        if not self.alpha_vantage_key:
            return []

        try:
            url = "https://www.alphavantage.co/query"
            params = {
                'function': 'NEWS_SENTIMENT',
                'tickers': ticker,
                'apikey': self.alpha_vantage_key,
                'limit': 50
            }

            response = requests.get(url, params=params, headers=self.headers)
            response.raise_for_status()

            data = response.json()
            news_items = []

            if 'feed' in data:
                for item in data['feed']:
                    # è§£ææ—¶é—´
                    time_str = item.get('time_published', '')
                    try:
                        publish_time = datetime.strptime(time_str, '%Y%m%dT%H%M%S').replace(tzinfo=ZoneInfo(get_timezone_name()))
                    except:
                        continue

                    # æ£€æŸ¥æ—¶æ•ˆæ€§
                    if publish_time < datetime.now(ZoneInfo(get_timezone_name())) - timedelta(hours=hours_back):
                        continue

                    urgency = self._assess_news_urgency(item.get('title', ''), item.get('summary', ''))

                    news_items.append(NewsItem(
                        title=item.get('title', ''),
                        content=item.get('summary', ''),
                        source=item.get('source', 'Alpha Vantage'),
                        publish_time=publish_time,
                        url=item.get('url', ''),
                        urgency=urgency,
                        relevance_score=self._calculate_relevance(item.get('title', ''), ticker)
                    ))

            return news_items

        except Exception as e:
            logger.error(f"Alpha Vantageæ–°é—»è·å–å¤±è´¥: {e}")
            return []

    def _get_newsapi_news(self, ticker: str, hours_back: int) -> List[NewsItem]:
        """è·å–NewsAPIæ–°é—»"""
        try:
            # æ„å»ºæœç´¢æŸ¥è¯¢
            company_names = {
                'AAPL': 'Apple',
                'TSLA': 'Tesla',
                'NVDA': 'NVIDIA',
                'MSFT': 'Microsoft',
                'GOOGL': 'Google'
            }

            query = f"{ticker} OR {company_names.get(ticker, ticker)}"

            url = "https://newsapi.org/v2/everything"
            params = {
                'q': query,
                'language': 'en',
                'sortBy': 'publishedAt',
                'from': (datetime.now(ZoneInfo(get_timezone_name())) - timedelta(hours=hours_back)).isoformat(),
                'apiKey': self.newsapi_key
            }

            response = requests.get(url, params=params, headers=self.headers)
            response.raise_for_status()

            data = response.json()
            news_items = []

            for item in data.get('articles', []):
                # è§£ææ—¶é—´
                time_str = item.get('publishedAt', '')
                try:
                    publish_time = datetime.fromisoformat(time_str.replace('Z', '+00:00'))
                except:
                    continue

                urgency = self._assess_news_urgency(item.get('title', ''), item.get('description', ''))

                news_items.append(NewsItem(
                    title=item.get('title', ''),
                    content=item.get('description', ''),
                    source=item.get('source', {}).get('name', 'NewsAPI'),
                    publish_time=publish_time,
                    url=item.get('url', ''),
                    urgency=urgency,
                    relevance_score=self._calculate_relevance(item.get('title', ''), ticker)
                ))

            return news_items

        except Exception as e:
            logger.error(f"NewsAPIæ–°é—»è·å–å¤±è´¥: {e}")
            return []

    def _get_yahoo_finance_news(self, ticker: str, hours_back: int) -> List[NewsItem]:
        """è·å– Yahoo Finance æ–°é—» (ä¸»è¦ç”¨äºæ¸¯è‚¡)"""
        try:
            import yfinance as yf
            
            logger.info(f"[Yahoo Finance] å¼€å§‹è·å– {ticker} çš„æ–°é—»")
            
            # è·å–è‚¡ç¥¨å¯¹è±¡
            stock = yf.Ticker(ticker)
            
            # è·å–æ–°é—»
            news = stock.news
            
            if not news:
                logger.info(f"[Yahoo Finance] {ticker} æœªè¿”å›æ–°é—»")
                return []
            
            logger.info(f"[Yahoo Finance] æˆåŠŸè·å– {len(news)} æ¡åŸå§‹æ–°é—»")
            
            news_items = []
            cutoff_time = datetime.now(ZoneInfo(get_timezone_name())) - timedelta(hours=hours_back)
            
            for item in news:
                try:
                    # è§£æå‘å¸ƒæ—¶é—´
                    pub_time = item.get('providerPublishTime', 0)
                    if pub_time:
                        publish_time = datetime.fromtimestamp(pub_time, tz=ZoneInfo(get_timezone_name()))
                    else:
                        # å¦‚æœæ²¡æœ‰æ—¶é—´,ä½¿ç”¨å½“å‰æ—¶é—´
                        publish_time = datetime.now(ZoneInfo(get_timezone_name()))
                    
                    # æ£€æŸ¥æ—¶æ•ˆæ€§
                    if publish_time < cutoff_time:
                        continue
                    
                    # è·å–æ–°é—»å†…å®¹
                    title = item.get('title', '')
                    if not title:
                        continue
                    
                    # è·å–æ‘˜è¦æˆ–ä½¿ç”¨æ ‡é¢˜
                    content = item.get('summary', '') or title
                    
                    # è¯„ä¼°ç´§æ€¥ç¨‹åº¦
                    urgency = self._assess_news_urgency(title, content)
                    
                    # è·å–æ¥æº
                    source = item.get('publisher', 'Yahoo Finance')
                    
                    news_items.append(NewsItem(
                        title=title,
                        content=content,
                        source=source,
                        publish_time=publish_time,
                        url=item.get('link', ''),
                        urgency=urgency,
                        relevance_score=self._calculate_relevance(title, ticker)
                    ))
                    
                except Exception as item_e:
                    logger.debug(f"[Yahoo Finance] å¤„ç†æ–°é—»é¡¹å¤±è´¥: {item_e}")
                    continue
            
            logger.info(f"[Yahoo Finance] æˆåŠŸå¤„ç† {len(news_items)} æ¡æ–°é—» (æ—¶æ•ˆæ€§è¿‡æ»¤å)")
            return news_items
            
        except ImportError:
            logger.warning(f"[Yahoo Finance] yfinance åº“æœªå®‰è£…,è·³è¿‡æ­¤æ–°é—»æº")
            return []
        except Exception as e:
            logger.error(f"[Yahoo Finance] æ–°é—»è·å–å¤±è´¥: {e}")
            return []

    def _get_chinese_financial_media_news(self, hours_back: int = 24) -> List[NewsItem]:
        """
        è·å–ä¸­æ–‡è´¢ç»åª’ä½“æ–°é—» (é‡‘åæ•°æ®ã€åå°”è¡—è§é—»ã€æ ¼éš†æ±‡)
        è¿™äº›æ˜¯é€šç”¨è´¢ç»å¿«è®¯,ä¸é’ˆå¯¹ç‰¹å®šè‚¡ç¥¨
        
        é‡è¦: ä½¿ç”¨15åˆ†é’Ÿç¼“å­˜,é˜²æ­¢é¢‘ç¹è¯·æ±‚è¢«é‡‘åæ•°æ®æ‹‰é»‘IP
        """
        # æ£€æŸ¥ç¼“å­˜
        current_time = datetime.now(ZoneInfo(get_timezone_name()))
        
        if self._media_news_cache is not None and self._media_news_cache_time is not None:
            cache_age = current_time - self._media_news_cache_time
            if cache_age < self._media_cache_duration:
                logger.info(f"[ä¸­æ–‡è´¢ç»åª’ä½“] ä½¿ç”¨ç¼“å­˜æ•°æ® (ç¼“å­˜æ—¶é—´: {cache_age.total_seconds()/60:.1f}åˆ†é’Ÿ)")
                # è¿‡æ»¤æ—¶æ•ˆæ€§
                cutoff_time = current_time - timedelta(hours=hours_back)
                cached_news = [item for item in self._media_news_cache if item.publish_time >= cutoff_time]
                logger.info(f"[ä¸­æ–‡è´¢ç»åª’ä½“] ç¼“å­˜ä¸­æœ‰æ•ˆæ–°é—»: {len(cached_news)} æ¡")
                return cached_news
        
        all_news = []
        
        # RSS æºé…ç½®
        rss_sources = [
            {
                'name': 'é‡‘åæ•°æ®',
                'url': 'https://rss.cdx.hidns.co/jin10/flash',
                'priority': 1  # é«˜ä¼˜å…ˆçº§
            },
            {
                'name': 'åå°”è¡—è§é—»',
                'url': 'https://rss.cdx.hidns.co/wallstreetcn/live/global',
                'priority': 1  # é«˜ä¼˜å…ˆçº§
            },
            {
                'name': 'æ ¼éš†æ±‡',
                'url': 'https://rss.cdx.hidns.co/gelonghui/live',
                'priority': 2  # ä¸­ä¼˜å…ˆçº§
            },
            {
                'name': 'è´¢è”ç¤¾ç”µæŠ¥',
                'url': 'https://rss.cdx.hidns.co/cls/telegraph',
                'priority': 1  # é«˜ä¼˜å…ˆçº§ - Aè‚¡æ”¿ç­–æ¶ˆæ¯ç‰¹åˆ«å¿«
            }
        ]
        
        logger.info(f"[ä¸­æ–‡è´¢ç»åª’ä½“] å¼€å§‹è·å–è´¢ç»å¿«è®¯,å›æº¯æ—¶é—´: {hours_back}å°æ—¶")
        
        for source_config in rss_sources:
            source_name = source_config['name']
            source_url = source_config['url']
            
            try:
                logger.info(f"[ä¸­æ–‡è´¢ç»åª’ä½“] å°è¯•ä» {source_name} è·å–æ–°é—»")
                
                import feedparser
                response = requests.get(source_url, timeout=15, headers=self.headers)
                response.raise_for_status()
                
                feed = feedparser.parse(response.content)
                
                if not feed.entries:
                    logger.info(f"[ä¸­æ–‡è´¢ç»åª’ä½“] {source_name} æœªè¿”å›æ–°é—»")
                    continue
                
                logger.info(f"[ä¸­æ–‡è´¢ç»åª’ä½“] {source_name} è¿”å› {len(feed.entries)} æ¡æ–°é—»")
                
                cutoff_time = datetime.now(ZoneInfo(get_timezone_name())) - timedelta(hours=hours_back)
                processed_count = 0
                
                for entry in feed.entries:
                    try:
                        # è§£æå‘å¸ƒæ—¶é—´
                        pub_time_str = entry.get('published', '')
                        if pub_time_str:
                            try:
                                from email.utils import parsedate_to_datetime
                                publish_time = parsedate_to_datetime(pub_time_str)
                                # è½¬æ¢ä¸ºæœ¬åœ°æ—¶åŒº
                                publish_time = publish_time.astimezone(ZoneInfo(get_timezone_name()))
                            except:
                                publish_time = datetime.now(ZoneInfo(get_timezone_name()))
                        else:
                            publish_time = datetime.now(ZoneInfo(get_timezone_name()))
                        
                        # æ£€æŸ¥æ—¶æ•ˆæ€§
                        if publish_time < cutoff_time:
                            continue
                        
                        # è·å–æ ‡é¢˜å’Œå†…å®¹
                        title = entry.get('title', '').strip()
                        if not title:
                            continue
                        
                        # ç§»é™¤ HTML æ ‡ç­¾
                        import re
                        title = re.sub(r'<[^>]+>', '', title)
                        
                        content = entry.get('description', '') or entry.get('summary', '') or title
                        content = re.sub(r'<[^>]+>', '', content)
                        
                        # è¯„ä¼°ç´§æ€¥ç¨‹åº¦
                        urgency = self._assess_news_urgency(title, content)
                        
                        all_news.append(NewsItem(
                            title=title,
                            content=content,
                            source=source_name,
                            publish_time=publish_time,
                            url=entry.get('link', ''),
                            urgency=urgency,
                            relevance_score=0.5  # é€šç”¨æ–°é—»,ä¸­ç­‰ç›¸å…³æ€§
                        ))
                        
                        processed_count += 1
                        
                    except Exception as item_e:
                        logger.debug(f"[ä¸­æ–‡è´¢ç»åª’ä½“] å¤„ç† {source_name} æ–°é—»é¡¹å¤±è´¥: {item_e}")
                        continue
                
                logger.info(f"[ä¸­æ–‡è´¢ç»åª’ä½“] {source_name} æˆåŠŸå¤„ç† {processed_count} æ¡æ–°é—»")
                
            except ImportError:
                logger.warning(f"[ä¸­æ–‡è´¢ç»åª’ä½“] feedparser åº“æœªå®‰è£…,è·³è¿‡ {source_name}")
                continue
            except Exception as e:
                logger.error(f"[ä¸­æ–‡è´¢ç»åª’ä½“] {source_name} è·å–å¤±è´¥: {e}")
                continue
        
        logger.info(f"[ä¸­æ–‡è´¢ç»åª’ä½“] æ€»å…±è·å– {len(all_news)} æ¡è´¢ç»å¿«è®¯")
        
        # æ›´æ–°ç¼“å­˜
        if all_news:
            self._media_news_cache = all_news
            self._media_news_cache_time = datetime.now(ZoneInfo(get_timezone_name()))
            logger.info(f"[ä¸­æ–‡è´¢ç»åª’ä½“] ç¼“å­˜å·²æ›´æ–°,ä¸‹æ¬¡åˆ·æ–°æ—¶é—´: {(self._media_news_cache_time + self._media_cache_duration).strftime('%H:%M:%S')}")
        
        return all_news

    def _get_chinese_finance_news(self, ticker: str, hours_back: int) -> List[NewsItem]:
        """è·å–ä¸­æ–‡è´¢ç»æ–°é—»"""
        # é›†æˆä¸­æ–‡è´¢ç»æ–°é—»APIï¼šè´¢è”ç¤¾ã€ä¸œæ–¹è´¢å¯Œç­‰
        logger.info(f"[ä¸­æ–‡è´¢ç»æ–°é—»] å¼€å§‹è·å– {ticker} çš„ä¸­æ–‡è´¢ç»æ–°é—»ï¼Œå›æº¯æ—¶é—´: {hours_back}å°æ—¶")
        start_time = datetime.now(ZoneInfo(get_timezone_name()))

        try:
            news_items = []

            # 1. å°è¯•ä½¿ç”¨AKShareè·å–ä¸œæ–¹è´¢å¯Œä¸ªè‚¡æ–°é—»
            try:
                logger.info(f"[ä¸­æ–‡è´¢ç»æ–°é—»] å°è¯•é€šè¿‡ AKShare Provider è·å–æ–°é—»")
                from tradingagents.dataflows.providers.china.akshare import AKShareProvider

                provider = AKShareProvider()

                # å¤„ç†è‚¡ç¥¨ä»£ç æ ¼å¼
                # å¦‚æœæ˜¯ç¾è‚¡ä»£ç ï¼Œä¸ä½¿ç”¨ä¸œæ–¹è´¢å¯Œæ–°é—»
                if '.' in ticker and any(suffix in ticker for suffix in ['.US', '.N', '.O', '.NYSE', '.NASDAQ']):
                    logger.info(f"[ä¸­æ–‡è´¢ç»æ–°é—»] æ£€æµ‹åˆ°ç¾è‚¡ä»£ç  {ticker}ï¼Œè·³è¿‡ä¸œæ–¹è´¢å¯Œæ–°é—»è·å–")
                else:
                    # å¤„ç†Aè‚¡å’Œæ¸¯è‚¡ä»£ç 
                    clean_ticker = ticker.replace('.SH', '').replace('.SZ', '').replace('.SS', '')\
                                    .replace('.HK', '').replace('.XSHE', '').replace('.XSHG', '')

                    # è·å–ä¸œæ–¹è´¢å¯Œæ–°é—»
                    logger.info(f"[ä¸­æ–‡è´¢ç»æ–°é—»] å¼€å§‹è·å– {clean_ticker} çš„ä¸œæ–¹è´¢å¯Œæ–°é—»")
                    em_start_time = datetime.now(ZoneInfo(get_timezone_name()))
                    news_df = provider.get_stock_news_sync(symbol=clean_ticker)

                    if not news_df.empty:
                        logger.info(f"[ä¸­æ–‡è´¢ç»æ–°é—»] ä¸œæ–¹è´¢å¯Œè¿”å› {len(news_df)} æ¡æ–°é—»æ•°æ®ï¼Œå¼€å§‹å¤„ç†")
                        processed_count = 0
                        skipped_count = 0
                        error_count = 0

                        # è½¬æ¢ä¸ºNewsItemæ ¼å¼
                        for _, row in news_df.iterrows():
                            try:
                                # è§£ææ—¶é—´
                                time_str = row.get('æ—¶é—´', '')
                                if time_str:
                                    # å°è¯•è§£ææ—¶é—´æ ¼å¼ï¼Œå¯èƒ½æ˜¯'2023-01-01 12:34:56'æ ¼å¼
                                    try:
                                        publish_time = datetime.strptime(time_str, '%Y-%m-%d %H:%M:%S').replace(tzinfo=ZoneInfo(get_timezone_name()))
                                    except:
                                        # å°è¯•å…¶ä»–å¯èƒ½çš„æ ¼å¼
                                        try:
                                            publish_time = datetime.strptime(time_str, '%Y-%m-%d').replace(tzinfo=ZoneInfo(get_timezone_name()))
                                        except:
                                            logger.warning(f"[ä¸­æ–‡è´¢ç»æ–°é—»] æ— æ³•è§£ææ—¶é—´æ ¼å¼: {time_str}ï¼Œä½¿ç”¨å½“å‰æ—¶é—´")
                                            publish_time = datetime.now(ZoneInfo(get_timezone_name()))
                                else:
                                    logger.warning(f"[ä¸­æ–‡è´¢ç»æ–°é—»] æ–°é—»æ—¶é—´ä¸ºç©ºï¼Œä½¿ç”¨å½“å‰æ—¶é—´")
                                    publish_time = datetime.now(ZoneInfo(get_timezone_name()))

                                # æ£€æŸ¥æ—¶æ•ˆæ€§
                                if publish_time < datetime.now(ZoneInfo(get_timezone_name())) - timedelta(hours=hours_back):
                                    skipped_count += 1
                                    continue

                                # è¯„ä¼°ç´§æ€¥ç¨‹åº¦
                                title = row.get('æ ‡é¢˜', '')
                                content = row.get('å†…å®¹', '')
                                urgency = self._assess_news_urgency(title, content)

                                news_items.append(NewsItem(
                                    title=title,
                                    content=content,
                                    source='ä¸œæ–¹è´¢å¯Œ',
                                    publish_time=publish_time,
                                    url=row.get('é“¾æ¥', ''),
                                    urgency=urgency,
                                    relevance_score=self._calculate_relevance(title, ticker)
                                ))
                                processed_count += 1
                            except Exception as item_e:
                                logger.error(f"[ä¸­æ–‡è´¢ç»æ–°é—»] å¤„ç†ä¸œæ–¹è´¢å¯Œæ–°é—»é¡¹ç›®å¤±è´¥: {item_e}")
                                error_count += 1
                                continue

                        em_time = (datetime.now(ZoneInfo(get_timezone_name())) - em_start_time).total_seconds()
                        logger.info(f"[ä¸­æ–‡è´¢ç»æ–°é—»] ä¸œæ–¹è´¢å¯Œæ–°é—»å¤„ç†å®Œæˆï¼ŒæˆåŠŸ: {processed_count}æ¡ï¼Œè·³è¿‡: {skipped_count}æ¡ï¼Œé”™è¯¯: {error_count}æ¡ï¼Œè€—æ—¶: {em_time:.2f}ç§’")
            except Exception as ak_e:
                logger.error(f"[ä¸­æ–‡è´¢ç»æ–°é—»] è·å–ä¸œæ–¹è´¢å¯Œæ–°é—»å¤±è´¥: {ak_e}")

            # 2. è´¢è”ç¤¾RSS (ä½¿ç”¨RSSHub)
            logger.info(f"[ä¸­æ–‡è´¢ç»æ–°é—»] å¼€å§‹è·å–è´¢è”ç¤¾RSSæ–°é—»")
            rss_start_time = datetime.now(ZoneInfo(get_timezone_name()))
            # ä½¿ç”¨ RSSHub æä¾›çš„è´¢è”ç¤¾æ–°é—»æº
            rss_sources = [
                "https://rsshub.app/cls/telegraph",  # è´¢è”ç¤¾ç”µæŠ¥å¿«è®¯ (ä¸»è¦)
                "https://rsshub.rssforever.com/cls/telegraph",  # å¤‡ç”¨ RSSHub å®ä¾‹
                # å¯ä»¥æ·»åŠ æ›´å¤šRSSæº
            ]

            rss_success_count = 0
            rss_error_count = 0
            total_rss_items = 0

            for rss_url in rss_sources:
                try:
                    logger.info(f"[ä¸­æ–‡è´¢ç»æ–°é—»] å°è¯•è§£æRSSæº: {rss_url}")
                    rss_item_start = datetime.now(ZoneInfo(get_timezone_name()))
                    items = self._parse_rss_feed(rss_url, ticker, hours_back)
                    rss_item_time = (datetime.now(ZoneInfo(get_timezone_name())) - rss_item_start).total_seconds()

                    if items:
                        logger.info(f"[ä¸­æ–‡è´¢ç»æ–°é—»] æˆåŠŸä»RSSæºè·å– {len(items)} æ¡æ–°é—»ï¼Œè€—æ—¶: {rss_item_time:.2f}ç§’")
                        news_items.extend(items)
                        total_rss_items += len(items)
                        rss_success_count += 1
                    else:
                        logger.info(f"[ä¸­æ–‡è´¢ç»æ–°é—»] RSSæºæœªè¿”å›ç›¸å…³æ–°é—»ï¼Œè€—æ—¶: {rss_item_time:.2f}ç§’")
                except Exception as rss_e:
                    logger.error(f"[ä¸­æ–‡è´¢ç»æ–°é—»] è§£æRSSæºå¤±è´¥: {rss_e}")
                    rss_error_count += 1
                    continue

            # è®°å½•RSSè·å–æ€»ç»“
            rss_total_time = (datetime.now(ZoneInfo(get_timezone_name())) - rss_start_time).total_seconds()
            logger.info(f"[ä¸­æ–‡è´¢ç»æ–°é—»] RSSæ–°é—»è·å–å®Œæˆï¼ŒæˆåŠŸæº: {rss_success_count}ä¸ªï¼Œå¤±è´¥æº: {rss_error_count}ä¸ªï¼Œè·å–æ–°é—»: {total_rss_items}æ¡ï¼Œæ€»è€—æ—¶: {rss_total_time:.2f}ç§’")

            # è®°å½•ä¸­æ–‡è´¢ç»æ–°é—»è·å–æ€»ç»“
            total_time = (datetime.now(ZoneInfo(get_timezone_name())) - start_time).total_seconds()
            logger.info(f"[ä¸­æ–‡è´¢ç»æ–°é—»] {ticker} çš„ä¸­æ–‡è´¢ç»æ–°é—»è·å–å®Œæˆï¼Œæ€»å…±è·å– {len(news_items)} æ¡æ–°é—»ï¼Œæ€»è€—æ—¶: {total_time:.2f}ç§’")

            return news_items

        except Exception as e:
            logger.error(f"[ä¸­æ–‡è´¢ç»æ–°é—»] ä¸­æ–‡è´¢ç»æ–°é—»è·å–å¤±è´¥: {e}")
            return []

    def _parse_rss_feed(self, rss_url: str, ticker: str, hours_back: int) -> List[NewsItem]:
        """è§£æRSSæº"""
        logger.info(f"[RSSè§£æ] å¼€å§‹è§£æRSSæº: {rss_url}ï¼Œè‚¡ç¥¨: {ticker}ï¼Œå›æº¯æ—¶é—´: {hours_back}å°æ—¶")
        start_time = datetime.now(ZoneInfo(get_timezone_name()))

        try:
            # å®é™…å®ç°éœ€è¦ä½¿ç”¨feedparseråº“
            # è¿™é‡Œæ˜¯ç®€åŒ–å®ç°ï¼Œå®é™…é¡¹ç›®ä¸­åº”è¯¥æ›¿æ¢ä¸ºçœŸå®çš„RSSè§£æé€»è¾‘
            import feedparser

            logger.info(f"[RSSè§£æ] å°è¯•è·å–RSSæºå†…å®¹")
            feed = feedparser.parse(rss_url)

            if not feed or not feed.entries:
                logger.warning(f"[RSSè§£æ] RSSæºæœªè¿”å›æœ‰æ•ˆå†…å®¹")
                return []

            logger.info(f"[RSSè§£æ] æˆåŠŸè·å–RSSæºï¼ŒåŒ…å« {len(feed.entries)} æ¡æ¡ç›®")
            news_items = []
            processed_count = 0
            skipped_count = 0

            for entry in feed.entries:
                try:
                    # è§£ææ—¶é—´
                    if hasattr(entry, 'published_parsed') and entry.published_parsed:
                        publish_time = datetime.fromtimestamp(time.mktime(entry.published_parsed), tz=ZoneInfo(get_timezone_name()))
                    else:
                        logger.warning(f"[RSSè§£æ] æ¡ç›®ç¼ºå°‘å‘å¸ƒæ—¶é—´ï¼Œä½¿ç”¨å½“å‰æ—¶é—´")
                        publish_time = datetime.now(ZoneInfo(get_timezone_name()))

                    # æ£€æŸ¥æ—¶æ•ˆæ€§
                    if publish_time < datetime.now(ZoneInfo(get_timezone_name())) - timedelta(hours=hours_back):
                        skipped_count += 1
                        continue

                    title = entry.title if hasattr(entry, 'title') else ''
                    content = entry.description if hasattr(entry, 'description') else ''

                    # è´¢è”ç¤¾æ˜¯é€šç”¨è´¢ç»å¿«è®¯,ä¸è¿›è¡Œä¸¥æ ¼çš„è‚¡ç¥¨ä»£ç è¿‡æ»¤
                    # è€Œæ˜¯ä¿ç•™æ‰€æœ‰æ–°é—»,é€šè¿‡ç›¸å…³æ€§è¯„åˆ†æ¥æ’åº
                    # è¿™æ ·å¯ä»¥è·å–æ›´å¤šè´¢ç»èµ„è®¯

                    # è¯„ä¼°ç´§æ€¥ç¨‹åº¦
                    urgency = self._assess_news_urgency(title, content)

                    news_items.append(NewsItem(
                        title=title,
                        content=content,
                        source='è´¢è”ç¤¾',
                        publish_time=publish_time,
                        url=entry.link if hasattr(entry, 'link') else '',
                        urgency=urgency,
                        relevance_score=self._calculate_relevance(title, ticker)
                    ))
                    processed_count += 1
                except Exception as e:
                    logger.error(f"[RSSè§£æ] å¤„ç†RSSæ¡ç›®å¤±è´¥: {e}")
                    continue

            total_time = (datetime.now(ZoneInfo(get_timezone_name())) - start_time).total_seconds()
            logger.info(f"[RSSè§£æ] RSSæºè§£æå®Œæˆï¼ŒæˆåŠŸ: {processed_count}æ¡ï¼Œè·³è¿‡: {skipped_count}æ¡ï¼Œè€—æ—¶: {total_time:.2f}ç§’")
            return news_items
        except ImportError:
            logger.error(f"[RSSè§£æ] feedparseråº“æœªå®‰è£…ï¼Œæ— æ³•è§£æRSSæº")
            return []
        except Exception as e:
            logger.error(f"[RSSè§£æ] è§£æRSSæºå¤±è´¥: {e}")
            return []

    def _assess_news_urgency(self, title: str, content: str) -> str:
        """è¯„ä¼°æ–°é—»ç´§æ€¥ç¨‹åº¦"""
        text = (title + ' ' + content).lower()

        # é«˜ç´§æ€¥åº¦å…³é”®è¯
        high_urgency_keywords = [
            'breaking', 'urgent', 'alert', 'emergency', 'halt', 'suspend',
            'çªå‘', 'ç´§æ€¥', 'æš‚åœ', 'åœç‰Œ', 'é‡å¤§'
        ]

        # ä¸­ç­‰ç´§æ€¥åº¦å…³é”®è¯
        medium_urgency_keywords = [
            'earnings', 'report', 'announce', 'launch', 'merger', 'acquisition',
            'è´¢æŠ¥', 'å‘å¸ƒ', 'å®£å¸ƒ', 'å¹¶è´­', 'æ”¶è´­'
        ]

        # æ£€æŸ¥é«˜ç´§æ€¥åº¦å…³é”®è¯
        for keyword in high_urgency_keywords:
            if keyword in text:
                logger.debug(f"[ç´§æ€¥åº¦è¯„ä¼°] æ£€æµ‹åˆ°é«˜ç´§æ€¥åº¦å…³é”®è¯ '{keyword}' åœ¨æ–°é—»ä¸­: {title[:50]}...")
                return 'high'

        # æ£€æŸ¥ä¸­ç­‰ç´§æ€¥åº¦å…³é”®è¯
        for keyword in medium_urgency_keywords:
            if keyword in text:
                logger.debug(f"[ç´§æ€¥åº¦è¯„ä¼°] æ£€æµ‹åˆ°ä¸­ç­‰ç´§æ€¥åº¦å…³é”®è¯ '{keyword}' åœ¨æ–°é—»ä¸­: {title[:50]}...")
                return 'medium'

        logger.debug(f"[ç´§æ€¥åº¦è¯„ä¼°] æœªæ£€æµ‹åˆ°ç´§æ€¥å…³é”®è¯ï¼Œè¯„ä¼°ä¸ºä½ç´§æ€¥åº¦: {title[:50]}...")
        return 'low'

    def _calculate_relevance(self, title: str, ticker: str) -> float:
        """è®¡ç®—æ–°é—»ç›¸å…³æ€§åˆ†æ•°"""
        text = title.lower()
        ticker_lower = ticker.lower()

        # åŸºç¡€ç›¸å…³æ€§ - è‚¡ç¥¨ä»£ç ç›´æ¥å‡ºç°åœ¨æ ‡é¢˜ä¸­
        if ticker_lower in text:
            logger.debug(f"[ç›¸å…³æ€§è®¡ç®—] è‚¡ç¥¨ä»£ç  {ticker} ç›´æ¥å‡ºç°åœ¨æ ‡é¢˜ä¸­ï¼Œç›¸å…³æ€§è¯„åˆ†: 1.0ï¼Œæ ‡é¢˜: {title[:50]}...")
            return 1.0

        # å…¬å¸åç§°åŒ¹é…
        company_names = {
            'aapl': ['apple', 'iphone', 'ipad', 'mac'],
            'tsla': ['tesla', 'elon musk', 'electric vehicle'],
            'nvda': ['nvidia', 'gpu', 'ai chip'],
            'msft': ['microsoft', 'windows', 'azure'],
            'googl': ['google', 'alphabet', 'search']
        }

        # æ£€æŸ¥å…¬å¸ç›¸å…³å…³é”®è¯
        if ticker_lower in company_names:
            for name in company_names[ticker_lower]:
                if name in text:
                    logger.debug(f"[ç›¸å…³æ€§è®¡ç®—] æ£€æµ‹åˆ°å…¬å¸ç›¸å…³å…³é”®è¯ '{name}' åœ¨æ ‡é¢˜ä¸­ï¼Œç›¸å…³æ€§è¯„åˆ†: 0.8ï¼Œæ ‡é¢˜: {title[:50]}...")
                    return 0.8

        # æå–è‚¡ç¥¨ä»£ç çš„çº¯æ•°å­—éƒ¨åˆ†ï¼ˆé€‚ç”¨äºä¸­å›½è‚¡ç¥¨ï¼‰
        pure_code = ''.join(filter(str.isdigit, ticker))
        if pure_code and pure_code in text:
            logger.debug(f"[ç›¸å…³æ€§è®¡ç®—] è‚¡ç¥¨ä»£ç æ•°å­—éƒ¨åˆ† {pure_code} å‡ºç°åœ¨æ ‡é¢˜ä¸­ï¼Œç›¸å…³æ€§è¯„åˆ†: 0.9ï¼Œæ ‡é¢˜: {title[:50]}...")
            return 0.9

        logger.debug(f"[ç›¸å…³æ€§è®¡ç®—] æœªæ£€æµ‹åˆ°æ˜ç¡®ç›¸å…³æ€§ï¼Œä½¿ç”¨é»˜è®¤è¯„åˆ†: 0.3ï¼Œæ ‡é¢˜: {title[:50]}...")
        return 0.3  # é»˜è®¤ç›¸å…³æ€§

    def _deduplicate_news(self, news_items: List[NewsItem]) -> List[NewsItem]:
        """å»é‡æ–°é—»"""
        logger.info(f"[æ–°é—»å»é‡] å¼€å§‹å¯¹ {len(news_items)} æ¡æ–°é—»è¿›è¡Œå»é‡å¤„ç†")
        start_time = datetime.now(ZoneInfo(get_timezone_name()))

        seen_titles = set()
        unique_news = []
        duplicate_count = 0
        short_title_count = 0

        for item in news_items:
            # ç®€å•çš„æ ‡é¢˜å»é‡
            title_key = item.title.lower().strip()

            # æ£€æŸ¥æ ‡é¢˜é•¿åº¦(ä¿®æ”¹ä¸º3ä»¥æ”¯æŒä¸­æ–‡æ–°é—»æ ‡é¢˜)
            if len(title_key) <= 3:
                logger.debug(f"[æ–°é—»å»é‡] è·³è¿‡æ ‡é¢˜è¿‡çŸ­çš„æ–°é—»: '{item.title}'ï¼Œæ¥æº: {item.source}")
                short_title_count += 1
                continue

            # æ£€æŸ¥æ˜¯å¦é‡å¤
            if title_key in seen_titles:
                logger.debug(f"[æ–°é—»å»é‡] æ£€æµ‹åˆ°é‡å¤æ–°é—»: '{item.title[:50]}...'ï¼Œæ¥æº: {item.source}")
                duplicate_count += 1
                continue

            # æ·»åŠ åˆ°ç»“æœé›†
            seen_titles.add(title_key)
            unique_news.append(item)

        # è®°å½•å»é‡ç»“æœ
        time_taken = (datetime.now(ZoneInfo(get_timezone_name())) - start_time).total_seconds()
        logger.info(f"[æ–°é—»å»é‡] å»é‡å®Œæˆï¼ŒåŸå§‹æ–°é—»: {len(news_items)}æ¡ï¼Œå»é‡å: {len(unique_news)}æ¡ï¼Œ")
        logger.info(f"[æ–°é—»å»é‡] å»é™¤é‡å¤: {duplicate_count}æ¡ï¼Œæ ‡é¢˜è¿‡çŸ­: {short_title_count}æ¡ï¼Œè€—æ—¶: {time_taken:.2f}ç§’")

        return unique_news

    def format_news_report(self, news_items: List[NewsItem], ticker: str) -> str:
        """æ ¼å¼åŒ–æ–°é—»æŠ¥å‘Š"""
        logger.info(f"[æ–°é—»æŠ¥å‘Š] å¼€å§‹ä¸º {ticker} ç”Ÿæˆæ–°é—»æŠ¥å‘Š")
        start_time = datetime.now(ZoneInfo(get_timezone_name()))

        if not news_items:
            logger.warning(f"[æ–°é—»æŠ¥å‘Š] æœªè·å–åˆ° {ticker} çš„å®æ—¶æ–°é—»æ•°æ®")
            return f"æœªè·å–åˆ°{ticker}çš„å®æ—¶æ–°é—»æ•°æ®ã€‚"

        # æŒ‰ç´§æ€¥ç¨‹åº¦åˆ†ç»„
        high_urgency = [n for n in news_items if n.urgency == 'high']
        medium_urgency = [n for n in news_items if n.urgency == 'medium']
        low_urgency = [n for n in news_items if n.urgency == 'low']

        # è®°å½•æ–°é—»åˆ†ç±»æƒ…å†µ
        logger.info(f"[æ–°é—»æŠ¥å‘Š] {ticker} æ–°é—»åˆ†ç±»ç»Ÿè®¡: é«˜ç´§æ€¥åº¦ {len(high_urgency)}æ¡, ä¸­ç´§æ€¥åº¦ {len(medium_urgency)}æ¡, ä½ç´§æ€¥åº¦ {len(low_urgency)}æ¡")

        # è®°å½•æ–°é—»æ¥æºåˆ†å¸ƒ
        news_sources = {}
        for item in news_items:
            source = item.source
            if source in news_sources:
                news_sources[source] += 1
            else:
                news_sources[source] = 1

        sources_info = ", ".join([f"{source}: {count}æ¡" for source, count in news_sources.items()])
        logger.info(f"[æ–°é—»æŠ¥å‘Š] {ticker} æ–°é—»æ¥æºåˆ†å¸ƒ: {sources_info}")

        report = f"# {ticker} å®æ—¶æ–°é—»åˆ†ææŠ¥å‘Š\n\n"
        report += f"ğŸ“… ç”Ÿæˆæ—¶é—´: {datetime.now(ZoneInfo(get_timezone_name())).strftime('%Y-%m-%d %H:%M:%S')}\n"
        report += f"ğŸ“Š æ–°é—»æ€»æ•°: {len(news_items)}æ¡\n\n"

        if high_urgency:
            report += "## ğŸš¨ ç´§æ€¥æ–°é—»\n\n"
            for news in high_urgency[:3]:  # æœ€å¤šæ˜¾ç¤º3æ¡
                report += f"### {news.title}\n"
                report += f"**æ¥æº**: {news.source} | **æ—¶é—´**: {news.publish_time.strftime('%H:%M')}\n"
                report += f"{news.content}\n\n"

        if medium_urgency:
            report += "## ğŸ“¢ é‡è¦æ–°é—»\n\n"
            for news in medium_urgency[:5]:  # æœ€å¤šæ˜¾ç¤º5æ¡
                report += f"### {news.title}\n"
                report += f"**æ¥æº**: {news.source} | **æ—¶é—´**: {news.publish_time.strftime('%H:%M')}\n"
                report += f"{news.content}\n\n"

        # æ·»åŠ æ—¶æ•ˆæ€§è¯´æ˜
        latest_news = max(news_items, key=lambda x: x.publish_time)
        time_diff = datetime.now(ZoneInfo(get_timezone_name())) - latest_news.publish_time

        report += f"\n## â° æ•°æ®æ—¶æ•ˆæ€§\n"
        report += f"æœ€æ–°æ–°é—»å‘å¸ƒäº: {time_diff.total_seconds() / 60:.0f}åˆ†é’Ÿå‰\n"

        if time_diff.total_seconds() < 1800:  # 30åˆ†é’Ÿå†…
            report += "ğŸŸ¢ æ•°æ®æ—¶æ•ˆæ€§: ä¼˜ç§€ (30åˆ†é’Ÿå†…)\n"
        elif time_diff.total_seconds() < 3600:  # 1å°æ—¶å†…
            report += "ğŸŸ¡ æ•°æ®æ—¶æ•ˆæ€§: è‰¯å¥½ (1å°æ—¶å†…)\n"
        else:
            report += "ğŸ”´ æ•°æ®æ—¶æ•ˆæ€§: ä¸€èˆ¬ (è¶…è¿‡1å°æ—¶)\n"

        # è®°å½•æŠ¥å‘Šç”Ÿæˆå®Œæˆä¿¡æ¯
        end_time = datetime.now(ZoneInfo(get_timezone_name()))
        time_taken = (end_time - start_time).total_seconds()
        report_length = len(report)

        logger.info(f"[æ–°é—»æŠ¥å‘Š] {ticker} æ–°é—»æŠ¥å‘Šç”Ÿæˆå®Œæˆï¼Œè€—æ—¶: {time_taken:.2f}ç§’ï¼ŒæŠ¥å‘Šé•¿åº¦: {report_length}å­—ç¬¦")

        # è®°å½•æ—¶æ•ˆæ€§ä¿¡æ¯
        time_diff_minutes = time_diff.total_seconds() / 60
        logger.info(f"[æ–°é—»æŠ¥å‘Š] {ticker} æ–°é—»æ—¶æ•ˆæ€§: æœ€æ–°æ–°é—»å‘å¸ƒäº {time_diff_minutes:.1f}åˆ†é’Ÿå‰")

        return report


def get_realtime_stock_news(ticker: str, curr_date: str, hours_back: int = 6) -> str:
    """
    è·å–å®æ—¶è‚¡ç¥¨æ–°é—»çš„ä¸»è¦æ¥å£å‡½æ•°
    """
    logger.info(f"[æ–°é—»åˆ†æ] ========== å‡½æ•°å…¥å£ ==========")
    logger.info(f"[æ–°é—»åˆ†æ] å‡½æ•°: get_realtime_stock_news")
    logger.info(f"[æ–°é—»åˆ†æ] å‚æ•°: ticker={ticker}, curr_date={curr_date}, hours_back={hours_back}")
    logger.info(f"[æ–°é—»åˆ†æ] å¼€å§‹è·å– {ticker} çš„å®æ—¶æ–°é—»ï¼Œæ—¥æœŸ: {curr_date}, å›æº¯æ—¶é—´: {hours_back}å°æ—¶")
    start_total_time = datetime.now(ZoneInfo(get_timezone_name()))
    logger.info(f"[æ–°é—»åˆ†æ] å¼€å§‹æ—¶é—´: {start_total_time.strftime('%Y-%m-%d %H:%M:%S.%f')[:-3]}")

    # åˆ¤æ–­è‚¡ç¥¨ç±»å‹
    logger.info(f"[æ–°é—»åˆ†æ] ========== æ­¥éª¤1: è‚¡ç¥¨ç±»å‹åˆ¤æ–­ ==========")
    stock_type = "æœªçŸ¥"
    is_china_stock = False
    logger.info(f"[æ–°é—»åˆ†æ] åŸå§‹ticker: {ticker}")

    if '.' in ticker:
        logger.info(f"[æ–°é—»åˆ†æ] æ£€æµ‹åˆ°tickeråŒ…å«ç‚¹å·ï¼Œè¿›è¡Œåç¼€åŒ¹é…")
        if any(suffix in ticker for suffix in ['.SH', '.SZ', '.SS', '.XSHE', '.XSHG']):
            stock_type = "Aè‚¡"
            is_china_stock = True
            logger.info(f"[æ–°é—»åˆ†æ] åŒ¹é…åˆ°Aè‚¡åç¼€ï¼Œè‚¡ç¥¨ç±»å‹: {stock_type}")
        elif '.HK' in ticker:
            stock_type = "æ¸¯è‚¡"
            logger.info(f"[æ–°é—»åˆ†æ] åŒ¹é…åˆ°æ¸¯è‚¡åç¼€ï¼Œè‚¡ç¥¨ç±»å‹: {stock_type}")
        elif any(suffix in ticker for suffix in ['.US', '.N', '.O', '.NYSE', '.NASDAQ']):
            stock_type = "ç¾è‚¡"
            logger.info(f"[æ–°é—»åˆ†æ] åŒ¹é…åˆ°ç¾è‚¡åç¼€ï¼Œè‚¡ç¥¨ç±»å‹: {stock_type}")
        else:
            logger.info(f"[æ–°é—»åˆ†æ] æœªåŒ¹é…åˆ°å·²çŸ¥åç¼€")
    else:
        logger.info(f"[æ–°é—»åˆ†æ] tickerä¸åŒ…å«ç‚¹å·ï¼Œå°è¯•ä½¿ç”¨StockUtilsåˆ¤æ–­")
        # å°è¯•ä½¿ç”¨StockUtilsåˆ¤æ–­è‚¡ç¥¨ç±»å‹
        try:
            from tradingagents.utils.stock_utils import StockUtils
            logger.info(f"[æ–°é—»åˆ†æ] æˆåŠŸå¯¼å…¥StockUtilsï¼Œå¼€å§‹åˆ¤æ–­è‚¡ç¥¨ç±»å‹")
            market_info = StockUtils.get_market_info(ticker)
            logger.info(f"[æ–°é—»åˆ†æ] StockUtilsè¿”å›å¸‚åœºä¿¡æ¯: {market_info}")
            if market_info['is_china']:
                stock_type = "Aè‚¡"
                is_china_stock = True
                logger.info(f"[æ–°é—»åˆ†æ] StockUtilsåˆ¤æ–­ä¸ºAè‚¡")
            elif market_info['is_hk']:
                stock_type = "æ¸¯è‚¡"
                logger.info(f"[æ–°é—»åˆ†æ] StockUtilsåˆ¤æ–­ä¸ºæ¸¯è‚¡")
            elif market_info['is_us']:
                stock_type = "ç¾è‚¡"
                logger.info(f"[æ–°é—»åˆ†æ] StockUtilsåˆ¤æ–­ä¸ºç¾è‚¡")
        except Exception as e:
            logger.warning(f"[æ–°é—»åˆ†æ] ä½¿ç”¨StockUtilsåˆ¤æ–­è‚¡ç¥¨ç±»å‹å¤±è´¥: {e}")

    logger.info(f"[æ–°é—»åˆ†æ] æœ€ç»ˆåˆ¤æ–­ç»“æœ - è‚¡ç¥¨ {ticker} ç±»å‹: {stock_type}, æ˜¯å¦Aè‚¡: {is_china_stock}")

    # å¯¹äºAè‚¡ï¼Œä¼˜å…ˆä½¿ç”¨ä¸œæ–¹è´¢å¯Œæ–°é—»æº
    if is_china_stock:
        logger.info(f"[æ–°é—»åˆ†æ] ========== æ­¥éª¤2: Aè‚¡ä¸œæ–¹è´¢å¯Œæ–°é—»è·å– ==========")
        logger.info(f"[æ–°é—»åˆ†æ] æ£€æµ‹åˆ°Aè‚¡è‚¡ç¥¨ {ticker}ï¼Œä¼˜å…ˆå°è¯•ä½¿ç”¨ä¸œæ–¹è´¢å¯Œæ–°é—»æº")
        try:
            logger.info(f"[æ–°é—»åˆ†æ] å°è¯•é€šè¿‡ AKShare Provider è·å–æ–°é—»")
            from tradingagents.dataflows.providers.china.akshare import AKShareProvider

            provider = AKShareProvider()
            logger.info(f"[æ–°é—»åˆ†æ] æˆåŠŸåˆ›å»º AKShare Provider å®ä¾‹")

            # å¤„ç†Aè‚¡ä»£ç 
            clean_ticker = ticker.replace('.SH', '').replace('.SZ', '').replace('.SS', '')\
                            .replace('.XSHE', '').replace('.XSHG', '')
            logger.info(f"[æ–°é—»åˆ†æ] åŸå§‹ticker: {ticker} -> æ¸…ç†åticker: {clean_ticker}")

            logger.info(f"[æ–°é—»åˆ†æ] å‡†å¤‡è°ƒç”¨ provider.get_stock_news_sync({clean_ticker})")
            logger.info(f"[æ–°é—»åˆ†æ] å¼€å§‹ä»ä¸œæ–¹è´¢å¯Œè·å– {clean_ticker} çš„æ–°é—»æ•°æ®")
            start_time = datetime.now(ZoneInfo(get_timezone_name()))
            logger.info(f"[æ–°é—»åˆ†æ] ä¸œæ–¹è´¢å¯ŒAPIè°ƒç”¨å¼€å§‹æ—¶é—´: {start_time.strftime('%Y-%m-%d %H:%M:%S.%f')[:-3]}")

            news_df = provider.get_stock_news_sync(symbol=clean_ticker, limit=10)

            end_time = datetime.now(ZoneInfo(get_timezone_name()))
            time_taken = (end_time - start_time).total_seconds()
            logger.info(f"[æ–°é—»åˆ†æ] ä¸œæ–¹è´¢å¯ŒAPIè°ƒç”¨ç»“æŸæ—¶é—´: {end_time.strftime('%Y-%m-%d %H:%M:%S.%f')[:-3]}")
            logger.info(f"[æ–°é—»åˆ†æ] ä¸œæ–¹è´¢å¯ŒAPIè°ƒç”¨è€—æ—¶: {time_taken:.2f}ç§’")
            logger.info(f"[æ–°é—»åˆ†æ] ä¸œæ–¹è´¢å¯ŒAPIè¿”å›æ•°æ®ç±»å‹: {type(news_df)}")

            if hasattr(news_df, 'empty'):
                logger.info(f"[æ–°é—»åˆ†æ] ä¸œæ–¹è´¢å¯ŒAPIè¿”å›DataFrameï¼Œæ˜¯å¦ä¸ºç©º: {news_df.empty}")
                if not news_df.empty:
                    logger.info(f"[æ–°é—»åˆ†æ] ä¸œæ–¹è´¢å¯ŒAPIè¿”å›DataFrameå½¢çŠ¶: {news_df.shape}")
                    logger.info(f"[æ–°é—»åˆ†æ] ä¸œæ–¹è´¢å¯ŒAPIè¿”å›DataFrameåˆ—å: {list(news_df.columns) if hasattr(news_df, 'columns') else 'æ— åˆ—å'}")
            else:
                logger.info(f"[æ–°é—»åˆ†æ] ä¸œæ–¹è´¢å¯ŒAPIè¿”å›æ•°æ®: {news_df}")

            if not news_df.empty:
                # æ„å»ºç®€å•çš„æ–°é—»æŠ¥å‘Š
                news_count = len(news_df)
                logger.info(f"[æ–°é—»åˆ†æ] æˆåŠŸè·å– {news_count} æ¡ä¸œæ–¹è´¢å¯Œæ–°é—»ï¼Œè€—æ—¶ {time_taken:.2f} ç§’")

                report = f"# {ticker} ä¸œæ–¹è´¢å¯Œæ–°é—»æŠ¥å‘Š\n\n"
                report += f"ğŸ“… ç”Ÿæˆæ—¶é—´: {datetime.now(ZoneInfo(get_timezone_name())).strftime('%Y-%m-%d %H:%M:%S')}\n"
                report += f"ğŸ“Š æ–°é—»æ€»æ•°: {news_count}æ¡\n"
                report += f"ğŸ•’ è·å–è€—æ—¶: {time_taken:.2f}ç§’\n\n"

                # è®°å½•ä¸€äº›æ–°é—»æ ‡é¢˜ç¤ºä¾‹
                sample_titles = [row.get('æ–°é—»æ ‡é¢˜', 'æ— æ ‡é¢˜') for _, row in news_df.head(3).iterrows()]
                logger.info(f"[æ–°é—»åˆ†æ] æ–°é—»æ ‡é¢˜ç¤ºä¾‹: {', '.join(sample_titles)}")

                logger.info(f"[æ–°é—»åˆ†æ] å¼€å§‹æ„å»ºæ–°é—»æŠ¥å‘Š")
                for idx, (_, row) in enumerate(news_df.iterrows()):
                    if idx < 3:  # åªè®°å½•å‰3æ¡çš„è¯¦ç»†ä¿¡æ¯
                        logger.info(f"[æ–°é—»åˆ†æ] ç¬¬{idx+1}æ¡æ–°é—»: æ ‡é¢˜={row.get('æ–°é—»æ ‡é¢˜', 'æ— æ ‡é¢˜')}, æ—¶é—´={row.get('å‘å¸ƒæ—¶é—´', 'æ— æ—¶é—´')}")
                    report += f"### {row.get('æ–°é—»æ ‡é¢˜', '')}\n"
                    report += f"ğŸ“… {row.get('å‘å¸ƒæ—¶é—´', '')}\n"
                    report += f"ğŸ”— {row.get('æ–°é—»é“¾æ¥', '')}\n\n"
                    report += f"{row.get('æ–°é—»å†…å®¹', 'æ— å†…å®¹')}\n\n"

                total_time_taken = (datetime.now(ZoneInfo(get_timezone_name())) - start_total_time).total_seconds()
                logger.info(f"[æ–°é—»åˆ†æ] æˆåŠŸç”Ÿæˆ {ticker} çš„æ–°é—»æŠ¥å‘Šï¼Œæ€»è€—æ—¶ {total_time_taken:.2f} ç§’ï¼Œæ–°é—»æ¥æº: ä¸œæ–¹è´¢å¯Œ")
                logger.info(f"[æ–°é—»åˆ†æ] æŠ¥å‘Šé•¿åº¦: {len(report)} å­—ç¬¦")
                logger.info(f"[æ–°é—»åˆ†æ] ========== ä¸œæ–¹è´¢å¯Œæ–°é—»è·å–æˆåŠŸï¼Œå‡½æ•°å³å°†è¿”å› ==========")
                return report
            else:
                logger.warning(f"[æ–°é—»åˆ†æ] ä¸œæ–¹è´¢å¯Œæœªè·å–åˆ° {ticker} çš„æ–°é—»ï¼Œè€—æ—¶ {time_taken:.2f} ç§’ï¼Œå°è¯•ä½¿ç”¨å…¶ä»–æ–°é—»æº")
        except Exception as e:
            logger.error(f"[æ–°é—»åˆ†æ] ä¸œæ–¹è´¢å¯Œæ–°é—»è·å–å¤±è´¥: {e}ï¼Œå°†å°è¯•å…¶ä»–æ–°é—»æº")
            logger.error(f"[æ–°é—»åˆ†æ] å¼‚å¸¸è¯¦æƒ…: {type(e).__name__}: {str(e)}")
            import traceback
            logger.error(f"[æ–°é—»åˆ†æ] å¼‚å¸¸å †æ ˆ: {traceback.format_exc()}")
    else:
        logger.info(f"[æ–°é—»åˆ†æ] ========== è·³è¿‡Aè‚¡ä¸œæ–¹è´¢å¯Œæ–°é—»è·å– ==========")
        logger.info(f"[æ–°é—»åˆ†æ] è‚¡ç¥¨ç±»å‹ä¸º {stock_type}ï¼Œä¸æ˜¯Aè‚¡ï¼Œè·³è¿‡ä¸œæ–¹è´¢å¯Œæ–°é—»æº")

    # å¦‚æœä¸æ˜¯Aè‚¡æˆ–Aè‚¡æ–°é—»è·å–å¤±è´¥ï¼Œä½¿ç”¨å®æ—¶æ–°é—»èšåˆå™¨
    logger.info(f"[æ–°é—»åˆ†æ] ========== æ­¥éª¤3: å®æ—¶æ–°é—»èšåˆå™¨ ==========")
    aggregator = RealtimeNewsAggregator()
    logger.info(f"[æ–°é—»åˆ†æ] æˆåŠŸåˆ›å»ºå®æ—¶æ–°é—»èšåˆå™¨å®ä¾‹")
    try:
        logger.info(f"[æ–°é—»åˆ†æ] å°è¯•ä½¿ç”¨å®æ—¶æ–°é—»èšåˆå™¨è·å– {ticker} çš„æ–°é—»")
        start_time = datetime.now(ZoneInfo(get_timezone_name()))
        logger.info(f"[æ–°é—»åˆ†æ] èšåˆå™¨è°ƒç”¨å¼€å§‹æ—¶é—´: {start_time.strftime('%Y-%m-%d %H:%M:%S.%f')[:-3]}")

        # è·å–å®æ—¶æ–°é—»
        news_items = aggregator.get_realtime_stock_news(ticker, hours_back, max_news=10)

        end_time = datetime.now(ZoneInfo(get_timezone_name()))
        time_taken = (end_time - start_time).total_seconds()
        logger.info(f"[æ–°é—»åˆ†æ] èšåˆå™¨è°ƒç”¨ç»“æŸæ—¶é—´: {end_time.strftime('%Y-%m-%d %H:%M:%S.%f')[:-3]}")
        logger.info(f"[æ–°é—»åˆ†æ] èšåˆå™¨è°ƒç”¨è€—æ—¶: {time_taken:.2f}ç§’")
        logger.info(f"[æ–°é—»åˆ†æ] èšåˆå™¨è¿”å›æ•°æ®ç±»å‹: {type(news_items)}")
        logger.info(f"[æ–°é—»åˆ†æ] èšåˆå™¨è¿”å›æ•°æ®: {news_items}")

        # å¦‚æœæˆåŠŸè·å–åˆ°æ–°é—»
        if news_items and len(news_items) > 0:
            news_count = len(news_items)
            logger.info(f"[æ–°é—»åˆ†æ] å®æ—¶æ–°é—»èšåˆå™¨æˆåŠŸè·å– {news_count} æ¡ {ticker} çš„æ–°é—»ï¼Œè€—æ—¶ {time_taken:.2f} ç§’")

            # è®°å½•ä¸€äº›æ–°é—»æ ‡é¢˜ç¤ºä¾‹
            sample_titles = [item.title for item in news_items[:3]]
            logger.info(f"[æ–°é—»åˆ†æ] æ–°é—»æ ‡é¢˜ç¤ºä¾‹: {', '.join(sample_titles)}")

            # æ ¼å¼åŒ–æŠ¥å‘Š
            logger.info(f"[æ–°é—»åˆ†æ] å¼€å§‹æ ¼å¼åŒ–æ–°é—»æŠ¥å‘Š")
            report = aggregator.format_news_report(news_items, ticker)
            logger.info(f"[æ–°é—»åˆ†æ] æŠ¥å‘Šæ ¼å¼åŒ–å®Œæˆï¼Œé•¿åº¦: {len(report)} å­—ç¬¦")

            total_time_taken = (datetime.now(ZoneInfo(get_timezone_name())) - start_total_time).total_seconds()
            logger.info(f"[æ–°é—»åˆ†æ] æˆåŠŸç”Ÿæˆ {ticker} çš„æ–°é—»æŠ¥å‘Šï¼Œæ€»è€—æ—¶ {total_time_taken:.2f} ç§’ï¼Œæ–°é—»æ¥æº: å®æ—¶æ–°é—»èšåˆå™¨")
            logger.info(f"[æ–°é—»åˆ†æ] ========== å®æ—¶æ–°é—»èšåˆå™¨è·å–æˆåŠŸï¼Œå‡½æ•°å³å°†è¿”å› ==========")
            return report
        else:
            logger.warning(f"[æ–°é—»åˆ†æ] å®æ—¶æ–°é—»èšåˆå™¨æœªè·å–åˆ° {ticker} çš„æ–°é—»ï¼Œè€—æ—¶ {time_taken:.2f} ç§’ï¼Œå°è¯•ä½¿ç”¨å¤‡ç”¨æ–°é—»æº")
            # å¦‚æœæ²¡æœ‰è·å–åˆ°æ–°é—»ï¼Œç»§ç»­å°è¯•å¤‡ç”¨æ–¹æ¡ˆ
    except Exception as e:
        logger.error(f"[æ–°é—»åˆ†æ] å®æ—¶æ–°é—»èšåˆå™¨è·å–å¤±è´¥: {e}ï¼Œå°†å°è¯•å¤‡ç”¨æ–°é—»æº")
        logger.error(f"[æ–°é—»åˆ†æ] å¼‚å¸¸è¯¦æƒ…: {type(e).__name__}: {str(e)}")
        import traceback
        logger.error(f"[æ–°é—»åˆ†æ] å¼‚å¸¸å †æ ˆ: {traceback.format_exc()}")
        # å‘ç”Ÿå¼‚å¸¸æ—¶ï¼Œç»§ç»­å°è¯•å¤‡ç”¨æ–¹æ¡ˆ

    # å¤‡ç”¨æ–¹æ¡ˆ1: å¯¹äºæ¸¯è‚¡ï¼Œä¼˜å…ˆå°è¯•ä½¿ç”¨ä¸œæ–¹è´¢å¯Œæ–°é—»ï¼ˆAè‚¡å·²åœ¨å‰é¢å¤„ç†ï¼‰
    if not is_china_stock and '.HK' in ticker:
        logger.info(f"[æ–°é—»åˆ†æ] æ£€æµ‹åˆ°æ¸¯è‚¡ä»£ç  {ticker}ï¼Œå°è¯•ä½¿ç”¨ä¸œæ–¹è´¢å¯Œæ–°é—»æº")
        try:
            from tradingagents.dataflows.providers.china.akshare import AKShareProvider

            provider = AKShareProvider()

            # å¤„ç†æ¸¯è‚¡ä»£ç 
            clean_ticker = ticker.replace('.HK', '')

            logger.info(f"[æ–°é—»åˆ†æ] å¼€å§‹ä»ä¸œæ–¹è´¢å¯Œè·å–æ¸¯è‚¡ {clean_ticker} çš„æ–°é—»æ•°æ®")
            start_time = datetime.now(ZoneInfo(get_timezone_name()))
            news_df = provider.get_stock_news_sync(symbol=clean_ticker, limit=10)
            end_time = datetime.now(ZoneInfo(get_timezone_name()))
            time_taken = (end_time - start_time).total_seconds()

            if not news_df.empty:
                # æ„å»ºç®€å•çš„æ–°é—»æŠ¥å‘Š
                news_count = len(news_df)
                logger.info(f"[æ–°é—»åˆ†æ] æˆåŠŸè·å– {news_count} æ¡ä¸œæ–¹è´¢å¯Œæ¸¯è‚¡æ–°é—»ï¼Œè€—æ—¶ {time_taken:.2f} ç§’")

                report = f"# {ticker} ä¸œæ–¹è´¢å¯Œæ–°é—»æŠ¥å‘Š\n\n"
                report += f"ğŸ“… ç”Ÿæˆæ—¶é—´: {datetime.now(ZoneInfo(get_timezone_name())).strftime('%Y-%m-%d %H:%M:%S')}\n"
                report += f"ğŸ“Š æ–°é—»æ€»æ•°: {news_count}æ¡\n"
                report += f"ğŸ•’ è·å–è€—æ—¶: {time_taken:.2f}ç§’\n\n"

                # è®°å½•ä¸€äº›æ–°é—»æ ‡é¢˜ç¤ºä¾‹
                sample_titles = [row.get('æ–°é—»æ ‡é¢˜', 'æ— æ ‡é¢˜') for _, row in news_df.head(3).iterrows()]
                logger.info(f"[æ–°é—»åˆ†æ] æ–°é—»æ ‡é¢˜ç¤ºä¾‹: {', '.join(sample_titles)}")

                for _, row in news_df.iterrows():
                    report += f"### {row.get('æ–°é—»æ ‡é¢˜', '')}\n"
                    report += f"ğŸ“… {row.get('å‘å¸ƒæ—¶é—´', '')}\n"
                    report += f"ğŸ”— {row.get('æ–°é—»é“¾æ¥', '')}\n\n"
                    report += f"{row.get('æ–°é—»å†…å®¹', 'æ— å†…å®¹')}\n\n"

                logger.info(f"[æ–°é—»åˆ†æ] æˆåŠŸç”Ÿæˆä¸œæ–¹è´¢å¯Œæ–°é—»æŠ¥å‘Šï¼Œæ–°é—»æ¥æº: ä¸œæ–¹è´¢å¯Œ")
                return report
            else:
                logger.warning(f"[æ–°é—»åˆ†æ] ä¸œæ–¹è´¢å¯Œæœªè·å–åˆ° {clean_ticker} çš„æ–°é—»æ•°æ®ï¼Œè€—æ—¶ {time_taken:.2f} ç§’ï¼Œå°è¯•ä¸‹ä¸€ä¸ªå¤‡ç”¨æ–¹æ¡ˆ")
        except Exception as e:
            logger.error(f"[æ–°é—»åˆ†æ] ä¸œæ–¹è´¢å¯Œæ–°é—»è·å–å¤±è´¥: {e}ï¼Œå°†å°è¯•ä¸‹ä¸€ä¸ªå¤‡ç”¨æ–¹æ¡ˆ")

    # å¤‡ç”¨æ–¹æ¡ˆ2: å°è¯•ä½¿ç”¨Googleæ–°é—»
    try:
        from tradingagents.dataflows.interface import get_google_news

        # æ ¹æ®è‚¡ç¥¨ç±»å‹æ„å»ºæœç´¢æŸ¥è¯¢
        if stock_type == "Aè‚¡":
            # Aè‚¡ä½¿ç”¨ä¸­æ–‡å…³é”®è¯
            clean_ticker = ticker.replace('.SH', '').replace('.SZ', '').replace('.SS', '')\
                           .replace('.XSHE', '').replace('.XSHG', '')
            search_query = f"{clean_ticker} è‚¡ç¥¨ å…¬å¸ è´¢æŠ¥ æ–°é—»"
            logger.info(f"[æ–°é—»åˆ†æ] å¼€å§‹ä»Googleè·å–Aè‚¡ {clean_ticker} çš„ä¸­æ–‡æ–°é—»æ•°æ®ï¼ŒæŸ¥è¯¢: {search_query}")
        elif stock_type == "æ¸¯è‚¡":
            # æ¸¯è‚¡ä½¿ç”¨ä¸­æ–‡å…³é”®è¯
            clean_ticker = ticker.replace('.HK', '')
            search_query = f"{clean_ticker} æ¸¯è‚¡ å…¬å¸"
            logger.info(f"[æ–°é—»åˆ†æ] å¼€å§‹ä»Googleè·å–æ¸¯è‚¡ {clean_ticker} çš„æ–°é—»æ•°æ®ï¼ŒæŸ¥è¯¢: {search_query}")
        else:
            # ç¾è‚¡ä½¿ç”¨è‹±æ–‡å…³é”®è¯
            search_query = f"{ticker} stock news"
            logger.info(f"[æ–°é—»åˆ†æ] å¼€å§‹ä»Googleè·å– {ticker} çš„æ–°é—»æ•°æ®ï¼ŒæŸ¥è¯¢: {search_query}")

        start_time = datetime.now(ZoneInfo(get_timezone_name()))
        google_news = get_google_news(search_query, curr_date, 1)
        end_time = datetime.now(ZoneInfo(get_timezone_name()))
        time_taken = (end_time - start_time).total_seconds()

        if google_news and len(google_news.strip()) > 0:
            # ä¼°ç®—è·å–çš„æ–°é—»æ•°é‡
            news_lines = google_news.strip().split('\n')
            news_count = sum(1 for line in news_lines if line.startswith('###'))

            logger.info(f"[æ–°é—»åˆ†æ] æˆåŠŸè·å– Google æ–°é—»ï¼Œä¼°è®¡ {news_count} æ¡æ–°é—»ï¼Œè€—æ—¶ {time_taken:.2f} ç§’")

            # è®°å½•ä¸€äº›æ–°é—»æ ‡é¢˜ç¤ºä¾‹
            sample_titles = [line.replace('### ', '') for line in news_lines if line.startswith('### ')][:3]
            if sample_titles:
                logger.info(f"[æ–°é—»åˆ†æ] æ–°é—»æ ‡é¢˜ç¤ºä¾‹: {', '.join(sample_titles)}")

            logger.info(f"[æ–°é—»åˆ†æ] æˆåŠŸç”Ÿæˆ Google æ–°é—»æŠ¥å‘Šï¼Œæ–°é—»æ¥æº: Google")
            return google_news
        else:
            logger.warning(f"[æ–°é—»åˆ†æ] Google æ–°é—»æœªè·å–åˆ° {ticker} çš„æ–°é—»æ•°æ®ï¼Œè€—æ—¶ {time_taken:.2f} ç§’")
    except Exception as e:
        logger.error(f"[æ–°é—»åˆ†æ] Google æ–°é—»è·å–å¤±è´¥: {e}ï¼Œæ‰€æœ‰å¤‡ç”¨æ–¹æ¡ˆå‡å·²å°è¯•")

    # æ‰€æœ‰æ–¹æ³•éƒ½å¤±è´¥ï¼Œè¿”å›é”™è¯¯ä¿¡æ¯
    total_time_taken = (datetime.now(ZoneInfo(get_timezone_name())) - start_total_time).total_seconds()
    logger.error(f"[æ–°é—»åˆ†æ] {ticker} çš„æ‰€æœ‰æ–°é—»è·å–æ–¹æ³•å‡å·²å¤±è´¥ï¼Œæ€»è€—æ—¶ {total_time_taken:.2f} ç§’")

    # è®°å½•è¯¦ç»†çš„å¤±è´¥ä¿¡æ¯
    failure_details = {
        "è‚¡ç¥¨ä»£ç ": ticker,
        "è‚¡ç¥¨ç±»å‹": stock_type,
        "åˆ†ææ—¥æœŸ": curr_date,
        "å›æº¯æ—¶é—´": f"{hours_back}å°æ—¶",
        "æ€»è€—æ—¶": f"{total_time_taken:.2f}ç§’"
    }
    logger.error(f"[æ–°é—»åˆ†æ] æ–°é—»è·å–å¤±è´¥è¯¦æƒ…: {failure_details}")

    return f"""
å®æ—¶æ–°é—»è·å–å¤±è´¥ - {ticker}
åˆ†ææ—¥æœŸ: {curr_date}

âŒ é”™è¯¯ä¿¡æ¯: æ‰€æœ‰å¯ç”¨çš„æ–°é—»æºéƒ½æœªèƒ½è·å–åˆ°ç›¸å…³æ–°é—»

ğŸ’¡ å¤‡ç”¨å»ºè®®:
1. æ£€æŸ¥ç½‘ç»œè¿æ¥å’ŒAPIå¯†é’¥é…ç½®
2. ä½¿ç”¨åŸºç¡€æ–°é—»åˆ†æä½œä¸ºå¤‡é€‰
3. å…³æ³¨å®˜æ–¹è´¢ç»åª’ä½“çš„æœ€æ–°æŠ¥é“
4. è€ƒè™‘ä½¿ç”¨ä¸“ä¸šé‡‘èç»ˆç«¯è·å–å®æ—¶æ–°é—»

æ³¨: å®æ—¶æ–°é—»è·å–ä¾èµ–å¤–éƒ¨APIæœåŠ¡çš„å¯ç”¨æ€§ã€‚
"""
