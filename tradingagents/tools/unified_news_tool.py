#!/usr/bin/env python3
import json
import requests
"""
ç»Ÿä¸€æ–°é—»åˆ†æå·¥å…·
æ•´åˆAè‚¡ã€æ¸¯è‚¡ã€ç¾è‚¡ç­‰ä¸åŒå¸‚åœºçš„æ–°é—»è·å–é€»è¾‘åˆ°ä¸€ä¸ªå·¥å…·å‡½æ•°ä¸­
è®©å¤§æ¨¡å‹åªéœ€è¦è°ƒç”¨ä¸€ä¸ªå·¥å…·å°±èƒ½è·å–æ‰€æœ‰ç±»å‹è‚¡ç¥¨çš„æ–°é—»æ•°æ®
"""

import logging
from datetime import datetime
import re

logger = logging.getLogger(__name__)

class UnifiedNewsAnalyzer:
    """ç»Ÿä¸€æ–°é—»åˆ†æå™¨ï¼Œæ•´åˆæ‰€æœ‰æ–°é—»è·å–é€»è¾‘"""
    
    def __init__(self, toolkit):
        """åˆå§‹åŒ–ç»Ÿä¸€æ–°é—»åˆ†æå™¨
        
        Args:
            toolkit: åŒ…å«å„ç§æ–°é—»è·å–å·¥å…·çš„å·¥å…·åŒ…
        """
        self.toolkit = toolkit
        
    def get_stock_news_unified(self, stock_code: str, max_news: int = 10, model_info: str = "") -> dict:
        """
        ç»Ÿä¸€æ–°é—»è·å–æ¥å£
        æ ¹æ®è‚¡ç¥¨ä»£ç è‡ªåŠ¨è¯†åˆ«è‚¡ç¥¨ç±»å‹å¹¶è·å–ç›¸åº”æ–°é—»
        
        Args:
            stock_code: è‚¡ç¥¨ä»£ç 
            max_news: æœ€å¤§æ–°é—»æ•°é‡
            model_info: å½“å‰ä½¿ç”¨çš„æ¨¡å‹ä¿¡æ¯ï¼Œç”¨äºç‰¹æ®Šå¤„ç†
            
        Returns:
            dict: åŒ…å«æ–°é—»å†…å®¹å’Œå…ƒæ•°æ®çš„å­—å…¸
        """
        logger.info(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] å¼€å§‹è·å– {stock_code} çš„æ–°é—»ï¼Œæ¨¡å‹: {model_info}")
        logger.info(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] ğŸ¤– å½“å‰æ¨¡å‹ä¿¡æ¯: {model_info}")
        
        # è¯†åˆ«è‚¡ç¥¨ç±»å‹
        stock_type = self._identify_stock_type(stock_code)
        logger.info(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] è‚¡ç¥¨ç±»å‹: {stock_type}")
        
        # æ ¹æ®è‚¡ç¥¨ç±»å‹è°ƒç”¨ç›¸åº”çš„è·å–æ–¹æ³•
        if stock_type == "Aè‚¡":
            result_str = self._get_a_share_news(stock_code, max_news, model_info)
        elif stock_type == "æ¸¯è‚¡":
            result_str = self._get_hk_share_news(stock_code, max_news, model_info)
        elif stock_type == "ç¾è‚¡":
            result_str = self._get_us_share_news(stock_code, max_news, model_info)
        else:
            # é»˜è®¤ä½¿ç”¨Aè‚¡é€»è¾‘
            result_str = self._get_a_share_news(stock_code, max_news, model_info)
        
        # ğŸ” æ·»åŠ è¯¦ç»†çš„ç»“æœè°ƒè¯•æ—¥å¿—
        logger.info(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] ğŸ“Š æ–°é—»è·å–å®Œæˆï¼Œç»“æœé•¿åº¦: {len(result_str)} å­—ç¬¦")
        logger.info(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] ğŸ“‹ è¿”å›ç»“æœé¢„è§ˆ (å‰1000å­—ç¬¦): {result_str[:1000]}")
        
        # å¦‚æœç»“æœä¸ºç©ºæˆ–è¿‡çŸ­ï¼Œè®°å½•è­¦å‘Š
        if not result_str or len(result_str.strip()) < 50:
            logger.warning(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] âš ï¸ è¿”å›ç»“æœå¼‚å¸¸çŸ­æˆ–ä¸ºç©ºï¼")
            logger.warning(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] ğŸ“ å®Œæ•´ç»“æœå†…å®¹: '{result_str}'")
        
        # æ„é€ è¿”å›å­—å…¸
        return {
            "status": "success" if result_str and len(result_str) > 50 else "warning",
            "content": result_str,
            "stock_type": stock_type,
            "ticker": stock_code,
            "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        }
    
    def _identify_stock_type(self, stock_code: str) -> str:
        """è¯†åˆ«è‚¡ç¥¨ç±»å‹"""
        stock_code = stock_code.upper().strip()
        
        # Aè‚¡åˆ¤æ–­
        if re.match(r'^(00|30|60|68)\d{4}$', stock_code):
            return "Aè‚¡"
        elif re.match(r'^(SZ|SH)\d{6}$', stock_code):
            return "Aè‚¡"
        
        # æ¸¯è‚¡åˆ¤æ–­
        elif re.match(r'^\d{4,5}\.HK$', stock_code):
            return "æ¸¯è‚¡"
        elif re.match(r'^\d{4,5}$', stock_code) and len(stock_code) <= 5:
            return "æ¸¯è‚¡"
        
        # ç¾è‚¡åˆ¤æ–­
        elif re.match(r'^[A-Z]{1,5}$', stock_code):
            return "ç¾è‚¡"
        elif '.' in stock_code and not stock_code.endswith('.HK'):
            return "ç¾è‚¡"
        
        # é»˜è®¤æŒ‰Aè‚¡å¤„ç†
        else:
            return "Aè‚¡"

    def _get_news_from_database(self, stock_code: str, max_news: int = 10) -> str:
        """
        ä»æ•°æ®åº“è·å–æ–°é—»

        Args:
            stock_code: è‚¡ç¥¨ä»£ç 
            max_news: æœ€å¤§æ–°é—»æ•°é‡

        Returns:
            str: æ ¼å¼åŒ–çš„æ–°é—»å†…å®¹ï¼Œå¦‚æœæ²¡æœ‰æ–°é—»åˆ™è¿”å›ç©ºå­—ç¬¦ä¸²
        """
        try:
            from tradingagents.dataflows.cache.app_adapter import get_mongodb_client
            from datetime import timedelta

            # ğŸ”§ ç¡®ä¿ max_news æ˜¯æ•´æ•°ï¼ˆé˜²æ­¢ä¼ å…¥æµ®ç‚¹æ•°ï¼‰
            max_news = int(max_news)

            client = get_mongodb_client()
            if not client:
                logger.warning(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] æ— æ³•è¿æ¥åˆ°MongoDB")
                return ""

            db = client.get_database('tradingagents')
            collection = db.stock_news

            # æ ‡å‡†åŒ–è‚¡ç¥¨ä»£ç ï¼ˆå»é™¤åç¼€ï¼‰
            clean_code = stock_code.replace('.SH', '').replace('.SZ', '').replace('.SS', '')\
                                   .replace('.XSHE', '').replace('.XSHG', '').replace('.HK', '')

            # æŸ¥è¯¢æœ€è¿‘30å¤©çš„æ–°é—»ï¼ˆæ‰©å¤§æ—¶é—´èŒƒå›´ï¼‰
            thirty_days_ago = datetime.now() - timedelta(days=30)

            # å°è¯•å¤šç§æŸ¥è¯¢æ–¹å¼ï¼ˆä½¿ç”¨ symbol å­—æ®µï¼‰
            query_list = [
                {'symbol': clean_code, 'publish_time': {'$gte': thirty_days_ago}},
                {'symbol': stock_code, 'publish_time': {'$gte': thirty_days_ago}},
                {'symbols': clean_code, 'publish_time': {'$gte': thirty_days_ago}},
                # å¦‚æœæœ€è¿‘30å¤©æ²¡æœ‰æ–°é—»ï¼Œåˆ™æŸ¥è¯¢æ‰€æœ‰æ–°é—»ï¼ˆä¸é™æ—¶é—´ï¼‰
                {'symbol': clean_code},
                {'symbols': clean_code},
            ]

            news_items = []
            for query in query_list:
                cursor = collection.find(query).sort('publish_time', -1).limit(max_news)
                news_items = list(cursor)
                if news_items:
                    logger.info(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] ğŸ“Š ä½¿ç”¨æŸ¥è¯¢ {query} æ‰¾åˆ° {len(news_items)} æ¡æ–°é—»")
                    break

            if not news_items:
                logger.info(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] æ•°æ®åº“ä¸­æ²¡æœ‰æ‰¾åˆ° {stock_code} çš„æ–°é—»")
                return ""

            # æ ¼å¼åŒ–æ–°é—»
            report = f"# {stock_code} æœ€æ–°æ–°é—» (æ•°æ®åº“ç¼“å­˜)\n\n"
            report += f"ğŸ“… æŸ¥è¯¢æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n"
            report += f"ğŸ“Š æ–°é—»æ•°é‡: {len(news_items)} æ¡\n\n"

            for i, news in enumerate(news_items, 1):
                title = news.get('title', 'æ— æ ‡é¢˜')
                content = news.get('content', '') or news.get('summary', '')
                source = news.get('source', 'æœªçŸ¥æ¥æº')
                publish_time = news.get('publish_time', datetime.now())
                sentiment = news.get('sentiment', 'neutral')

                # æƒ…ç»ªå›¾æ ‡
                sentiment_icon = {
                    'positive': 'ğŸ“ˆ',
                    'negative': 'ğŸ“‰',
                    'neutral': 'â–'
                }.get(sentiment, 'â–')

                report += f"## {i}. {sentiment_icon} {title}\n\n"
                report += f"**æ¥æº**: {source} | **æ—¶é—´**: {publish_time.strftime('%Y-%m-%d %H:%M') if isinstance(publish_time, datetime) else publish_time}\n"
                report += f"**æƒ…ç»ª**: {sentiment}\n\n"

                if content:
                    # é™åˆ¶å†…å®¹é•¿åº¦
                    content_preview = content[:500] + '...' if len(content) > 500 else content
                    report += f"{content_preview}\n\n"

                report += "---\n\n"

            logger.info(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] âœ… æˆåŠŸä»æ•°æ®åº“è·å–å¹¶æ ¼å¼åŒ– {len(news_items)} æ¡æ–°é—»")
            return report

        except Exception as e:
            logger.error(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] ä»æ•°æ®åº“è·å–æ–°é—»å¤±è´¥: {e}")
            import traceback
            logger.error(traceback.format_exc())
            return ""

    def _sync_news_from_akshare(self, stock_code: str, max_news: int = 10) -> bool:
        """
        ä»AKShareåŒæ­¥æ–°é—»åˆ°æ•°æ®åº“ï¼ˆåŒæ­¥æ–¹æ³•ï¼‰
        ä½¿ç”¨åŒæ­¥çš„æ•°æ®åº“å®¢æˆ·ç«¯å’Œæ–°çº¿ç¨‹ä¸­çš„äº‹ä»¶å¾ªç¯ï¼Œé¿å…äº‹ä»¶å¾ªç¯å†²çª

        Args:
            stock_code: è‚¡ç¥¨ä»£ç 
            max_news: æœ€å¤§æ–°é—»æ•°é‡

        Returns:
            bool: æ˜¯å¦åŒæ­¥æˆåŠŸ
        """
        try:
            import asyncio
            import concurrent.futures

            # æ ‡å‡†åŒ–è‚¡ç¥¨ä»£ç ï¼ˆå»é™¤åç¼€ï¼‰
            clean_code = stock_code.replace('.SH', '').replace('.SZ', '').replace('.SS', '')\
                                   .replace('.XSHE', '').replace('.XSHG', '').replace('.HK', '')

            logger.info(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] ğŸ”„ å¼€å§‹åŒæ­¥ {clean_code} çš„æ–°é—»...")

            # ğŸ”¥ åœ¨æ–°çº¿ç¨‹ä¸­è¿è¡Œï¼Œä½¿ç”¨åŒæ­¥æ•°æ®åº“å®¢æˆ·ç«¯
            def run_sync_in_new_thread():
                """åœ¨æ–°çº¿ç¨‹ä¸­åˆ›å»ºæ–°çš„äº‹ä»¶å¾ªç¯å¹¶è¿è¡ŒåŒæ­¥ä»»åŠ¡"""
                # åˆ›å»ºæ–°çš„äº‹ä»¶å¾ªç¯
                new_loop = asyncio.new_event_loop()
                asyncio.set_event_loop(new_loop)

                try:
                    # å®šä¹‰å¼‚æ­¥è·å–æ–°é—»ä»»åŠ¡
                    async def get_news_task():
                        try:
                            # åŠ¨æ€å¯¼å…¥ AKShare providerï¼ˆæ­£ç¡®çš„å¯¼å…¥è·¯å¾„ï¼‰
                            from tradingagents.dataflows.providers.china.akshare import AKShareProvider

                            # åˆ›å»º provider å®ä¾‹
                            provider = AKShareProvider()

                            # è°ƒç”¨ provider è·å–æ–°é—»
                            news_data = await provider.get_stock_news(
                                symbol=clean_code,
                                limit=max_news
                            )

                            return news_data

                        except Exception as e:
                            logger.error(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] âŒ è·å–æ–°é—»å¤±è´¥: {e}")
                            import traceback
                            logger.error(traceback.format_exc())
                            return None

                    # åœ¨æ–°çš„äº‹ä»¶å¾ªç¯ä¸­è·å–æ–°é—»
                    news_data = new_loop.run_until_complete(get_news_task())

                    if not news_data:
                        logger.warning(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] âš ï¸ æœªè·å–åˆ°æ–°é—»æ•°æ®")
                        return False

                    logger.info(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] ğŸ“¥ è·å–åˆ° {len(news_data)} æ¡æ–°é—»")

                    # ğŸ”¥ ä½¿ç”¨åŒæ­¥æ–¹æ³•ä¿å­˜åˆ°æ•°æ®åº“ï¼ˆä¸ä¾èµ–äº‹ä»¶å¾ªç¯ï¼‰
                    from app.services.news_data_service import NewsDataService

                    news_service = NewsDataService()
                    saved_count = news_service.save_news_data_sync(
                        news_data=news_data,
                        data_source="akshare",
                        market="CN"
                    )

                    logger.info(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] âœ… åŒæ­¥æˆåŠŸ: {saved_count} æ¡æ–°é—»")
                    return saved_count > 0

                finally:
                    # æ¸…ç†äº‹ä»¶å¾ªç¯
                    new_loop.close()

            # åœ¨çº¿ç¨‹æ± ä¸­æ‰§è¡Œ
            logger.info(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] åœ¨æ–°çº¿ç¨‹ä¸­è¿è¡ŒåŒæ­¥ä»»åŠ¡ï¼Œé¿å…äº‹ä»¶å¾ªç¯å†²çª")
            with concurrent.futures.ThreadPoolExecutor(max_workers=1) as executor:
                future = executor.submit(run_sync_in_new_thread)
                result = future.result(timeout=30)  # 30ç§’è¶…æ—¶
                return result

        except concurrent.futures.TimeoutError:
            logger.error(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] âŒ åŒæ­¥æ–°é—»è¶…æ—¶ï¼ˆ30ç§’ï¼‰")
            return False
        except Exception as e:
            logger.error(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] âŒ åŒæ­¥æ–°é—»å¤±è´¥: {e}")
            import traceback
            logger.error(traceback.format_exc())
            return False

    def _get_a_share_news(self, stock_code: str, max_news: int, model_info: str = "") -> str:
        """è·å–Aè‚¡æ–°é—»"""
        logger.info(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] è·å–Aè‚¡ {stock_code} æ–°é—»")

        # è·å–å½“å‰æ—¥æœŸ
        curr_date = datetime.now().strftime("%Y-%m-%d")

        # ä¼˜å…ˆçº§0: ä»æ•°æ®åº“è·å–æ–°é—»ï¼ˆæœ€é«˜ä¼˜å…ˆçº§ï¼‰
        try:
            logger.info(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] ğŸ” ä¼˜å…ˆä»æ•°æ®åº“è·å– {stock_code} çš„æ–°é—»...")
            db_news = self._get_news_from_database(stock_code, max_news)
            if db_news:
                logger.info(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] âœ… æ•°æ®åº“æ–°é—»è·å–æˆåŠŸ: {len(db_news)} å­—ç¬¦")
                return self._format_news_result(db_news, "æ•°æ®åº“ç¼“å­˜", model_info)
            else:
                logger.info(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] âš ï¸ æ•°æ®åº“ä¸­æ²¡æœ‰ {stock_code} çš„æ–°é—»ï¼Œå°è¯•åŒæ­¥...")

                # ğŸ”¥ æ•°æ®åº“æ²¡æœ‰æ•°æ®æ—¶ï¼Œè°ƒç”¨åŒæ­¥æœåŠ¡åŒæ­¥æ–°é—»
                try:
                    logger.info(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] ğŸ“¡ è°ƒç”¨åŒæ­¥æœåŠ¡åŒæ­¥ {stock_code} çš„æ–°é—»...")
                    synced_news = self._sync_news_from_akshare(stock_code, max_news)

                    if synced_news:
                        logger.info(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] âœ… åŒæ­¥æˆåŠŸï¼Œé‡æ–°ä»æ•°æ®åº“è·å–...")
                        # é‡æ–°ä»æ•°æ®åº“è·å–
                        db_news = self._get_news_from_database(stock_code, max_news)
                        if db_news:
                            logger.info(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] âœ… åŒæ­¥åæ•°æ®åº“æ–°é—»è·å–æˆåŠŸ: {len(db_news)} å­—ç¬¦")
                            return self._format_news_result(db_news, "æ•°æ®åº“ç¼“å­˜(æ–°åŒæ­¥)", model_info)
                    else:
                        logger.warning(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] âš ï¸ åŒæ­¥æœåŠ¡æœªè¿”å›æ–°é—»æ•°æ®")

                except Exception as sync_error:
                    logger.warning(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] âš ï¸ åŒæ­¥æœåŠ¡è°ƒç”¨å¤±è´¥: {sync_error}")

                logger.info(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] âš ï¸ åŒæ­¥åä»æ— æ•°æ®ï¼Œå°è¯•å…¶ä»–æ•°æ®æº...")
        except Exception as e:
            logger.warning(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] æ•°æ®åº“æ–°é—»è·å–å¤±è´¥: {e}")

        # ä¼˜å…ˆçº§1: ä¸œæ–¹è´¢å¯Œå®æ—¶æ–°é—»
        try:
            if hasattr(self.toolkit, 'get_realtime_stock_news'):
                logger.info(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] å°è¯•ä¸œæ–¹è´¢å¯Œå®æ—¶æ–°é—»...")
                # ä½¿ç”¨LangChainå·¥å…·çš„æ­£ç¡®è°ƒç”¨æ–¹å¼ï¼š.invoke()æ–¹æ³•å’Œå­—å…¸å‚æ•°
                result = self.toolkit.get_realtime_stock_news.invoke({"ticker": stock_code, "curr_date": curr_date})
                
                # ğŸ” è¯¦ç»†è®°å½•ä¸œæ–¹è´¢å¯Œè¿”å›çš„å†…å®¹
                logger.info(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] ğŸ“Š ä¸œæ–¹è´¢å¯Œè¿”å›å†…å®¹é•¿åº¦: {len(result) if result else 0} å­—ç¬¦")
                logger.info(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] ğŸ“‹ ä¸œæ–¹è´¢å¯Œè¿”å›å†…å®¹é¢„è§ˆ (å‰500å­—ç¬¦): {result[:500] if result else 'None'}")
                
                if result and len(result.strip()) > 100:
                    logger.info(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] âœ… ä¸œæ–¹è´¢å¯Œæ–°é—»è·å–æˆåŠŸ: {len(result)} å­—ç¬¦")
                    return self._format_news_result(result, "ä¸œæ–¹è´¢å¯Œå®æ—¶æ–°é—»", model_info)
                else:
                    logger.warning(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] âš ï¸ ä¸œæ–¹è´¢å¯Œæ–°é—»å†…å®¹è¿‡çŸ­æˆ–ä¸ºç©º")
        except Exception as e:
            logger.warning(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] ä¸œæ–¹è´¢å¯Œæ–°é—»è·å–å¤±è´¥: {e}")
        
        # ä¼˜å…ˆçº§2: Googleæ–°é—»ï¼ˆä¸­æ–‡æœç´¢ï¼‰
        try:
            if hasattr(self.toolkit, 'get_google_news'):
                logger.info(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] å°è¯•Googleæ–°é—»...")
                query = f"{stock_code} è‚¡ç¥¨ æ–°é—» è´¢æŠ¥ ä¸šç»©"
                # ä½¿ç”¨LangChainå·¥å…·çš„æ­£ç¡®è°ƒç”¨æ–¹å¼ï¼š.invoke()æ–¹æ³•å’Œå­—å…¸å‚æ•°
                result = self.toolkit.get_google_news.invoke({"query": query, "curr_date": curr_date})
                if result and len(result.strip()) > 50:
                    logger.info(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] âœ… Googleæ–°é—»è·å–æˆåŠŸ: {len(result)} å­—ç¬¦")
                    return self._format_news_result(result, "Googleæ–°é—»", model_info)
        except Exception as e:
            logger.warning(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] Googleæ–°é—»è·å–å¤±è´¥: {e}")
        
        # ä¼˜å…ˆçº§3: OpenAIå…¨çƒæ–°é—»
        try:
            if hasattr(self.toolkit, 'get_global_news_openai'):
                logger.info(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] å°è¯•OpenAIå…¨çƒæ–°é—»...")
                # ä½¿ç”¨LangChainå·¥å…·çš„æ­£ç¡®è°ƒç”¨æ–¹å¼ï¼š.invoke()æ–¹æ³•å’Œå­—å…¸å‚æ•°
                result = self.toolkit.get_global_news_openai.invoke({"curr_date": curr_date})
                if result and len(result.strip()) > 50:
                    logger.info(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] âœ… OpenAIæ–°é—»è·å–æˆåŠŸ: {len(result)} å­—ç¬¦")
                    return self._format_news_result(result, "OpenAIå…¨çƒæ–°é—»", model_info)
        except Exception as e:
            logger.warning(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] OpenAIæ–°é—»è·å–å¤±è´¥: {e}")
        
        return "âŒ æ— æ³•è·å–Aè‚¡æ–°é—»æ•°æ®ï¼Œæ‰€æœ‰æ–°é—»æºå‡ä¸å¯ç”¨"
    
    def _get_hk_share_news(self, stock_code: str, max_news: int, model_info: str = "") -> str:
        """è·å–æ¸¯è‚¡æ–°é—»"""
        logger.info(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] è·å–æ¸¯è‚¡ {stock_code} æ–°é—»")
        
        # è·å–å½“å‰æ—¥æœŸ
        curr_date = datetime.now().strftime("%Y-%m-%d")
        
        # ä¼˜å…ˆçº§1: Googleæ–°é—»ï¼ˆæ¸¯è‚¡æœç´¢ï¼‰
        try:
            if hasattr(self.toolkit, 'get_google_news'):
                logger.info(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] å°è¯•Googleæ¸¯è‚¡æ–°é—»...")
                query = f"{stock_code} æ¸¯è‚¡ é¦™æ¸¯è‚¡ç¥¨ æ–°é—»"
                # ä½¿ç”¨LangChainå·¥å…·çš„æ­£ç¡®è°ƒç”¨æ–¹å¼ï¼š.invoke()æ–¹æ³•å’Œå­—å…¸å‚æ•°
                result = self.toolkit.get_google_news.invoke({"query": query, "curr_date": curr_date})
                if result and len(result.strip()) > 50:
                    logger.info(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] âœ… Googleæ¸¯è‚¡æ–°é—»è·å–æˆåŠŸ: {len(result)} å­—ç¬¦")
                    return self._format_news_result(result, "Googleæ¸¯è‚¡æ–°é—»", model_info)
        except Exception as e:
            logger.warning(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] Googleæ¸¯è‚¡æ–°é—»è·å–å¤±è´¥: {e}")
        
        # ä¼˜å…ˆçº§2: OpenAIå…¨çƒæ–°é—»
        try:
            if hasattr(self.toolkit, 'get_global_news_openai'):
                logger.info(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] å°è¯•OpenAIæ¸¯è‚¡æ–°é—»...")
                # ä½¿ç”¨LangChainå·¥å…·çš„æ­£ç¡®è°ƒç”¨æ–¹å¼ï¼š.invoke()æ–¹æ³•å’Œå­—å…¸å‚æ•°
                result = self.toolkit.get_global_news_openai.invoke({"curr_date": curr_date})
                if result and len(result.strip()) > 50:
                    logger.info(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] âœ… OpenAIæ¸¯è‚¡æ–°é—»è·å–æˆåŠŸ: {len(result)} å­—ç¬¦")
                    return self._format_news_result(result, "OpenAIæ¸¯è‚¡æ–°é—»", model_info)
        except Exception as e:
            logger.warning(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] OpenAIæ¸¯è‚¡æ–°é—»è·å–å¤±è´¥: {e}")
        
        # ä¼˜å…ˆçº§3: å®æ—¶æ–°é—»ï¼ˆå¦‚æœæ”¯æŒæ¸¯è‚¡ï¼‰
        try:
            if hasattr(self.toolkit, 'get_realtime_stock_news'):
                logger.info(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] å°è¯•å®æ—¶æ¸¯è‚¡æ–°é—»...")
                # ä½¿ç”¨LangChainå·¥å…·çš„æ­£ç¡®è°ƒç”¨æ–¹å¼ï¼š.invoke()æ–¹æ³•å’Œå­—å…¸å‚æ•°
                result = self.toolkit.get_realtime_stock_news.invoke({"ticker": stock_code, "curr_date": curr_date})
                if result and len(result.strip()) > 100:
                    logger.info(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] âœ… å®æ—¶æ¸¯è‚¡æ–°é—»è·å–æˆåŠŸ: {len(result)} å­—ç¬¦")
                    return self._format_news_result(result, "å®æ—¶æ¸¯è‚¡æ–°é—»", model_info)
        except Exception as e:
            logger.warning(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] å®æ—¶æ¸¯è‚¡æ–°é—»è·å–å¤±è´¥: {e}")
        
        return "âŒ æ— æ³•è·å–æ¸¯è‚¡æ–°é—»æ•°æ®ï¼Œæ‰€æœ‰æ–°é—»æºå‡ä¸å¯ç”¨"
    
    def get_stock_sentiment_unified(
        self,
        ticker: str,
        curr_date: str
    ) -> dict:
        """
        ç»Ÿä¸€çš„è‚¡ç¥¨æƒ…ç»ªåˆ†æå·¥å…·
        è‡ªåŠ¨è¯†åˆ«è‚¡ç¥¨ç±»å‹ï¼ˆAè‚¡ã€æ¸¯è‚¡ã€ç¾è‚¡ï¼‰å¹¶è°ƒç”¨ç›¸åº”çš„æƒ…ç»ªæ•°æ®æº
        å¯¹äºAè‚¡å’Œæ¸¯è‚¡ï¼Œä½¿ç”¨Serper APIæŠ“å–é›ªçƒå’Œè‚¡å§çš„çœŸå®æ•£æˆ·è¯„è®º

        Args:
            ticker: è‚¡ç¥¨ä»£ç ï¼ˆå¦‚ï¼š000001ã€0700.HKã€AAPLï¼‰
            curr_date: å½“å‰æ—¥æœŸï¼ˆæ ¼å¼ï¼šYYYY-MM-DDï¼‰

        Returns:
            dict: åŒ…å«æƒ…ç»ªåˆ†ææŠ¥å‘Šå’Œå…ƒæ•°æ®çš„å­—å…¸
        """
        logger.info(f"ğŸ˜Š [ç»Ÿä¸€æƒ…ç»ªå·¥å…·] åˆ†æè‚¡ç¥¨: {ticker}")

        try:
            from tradingagents.utils.stock_utils import StockUtils

            # è‡ªåŠ¨è¯†åˆ«è‚¡ç¥¨ç±»å‹
            market_info = StockUtils.get_market_info(ticker)
            is_china = market_info['is_china']
            is_hk = market_info['is_hk']
            is_us = market_info['is_us']

            logger.info(f"ğŸ˜Š [ç»Ÿä¸€æƒ…ç»ªå·¥å…·] è‚¡ç¥¨ç±»å‹: {market_info['market_name']}")

            result_data = []
            
            # åˆå§‹åŒ–é»˜è®¤è¿”å›å­—å…¸
            response_dict = {
                "ticker": ticker,
                "stock_type": market_info['market_name'],
                "date": curr_date,
                "sentiment": "Neutral",
                "score": 0.5,
                "summary": "åˆ†æä¸­...",
                "confidence": "low",
                "content": ""
            }

            if is_china or is_hk:
                # ä¸­å›½Aè‚¡å’Œæ¸¯è‚¡ï¼šä½¿ç”¨Serper APIæœç´¢é›ªçƒå’Œè‚¡å§
                logger.info(f"ğŸ‡¨ğŸ‡³ğŸ‡­ğŸ‡° [ç»Ÿä¸€æƒ…ç»ªå·¥å…·] ä½¿ç”¨Serperæœç´¢ä¸­æ–‡å¸‚åœºæƒ…ç»ª...")
                
                try:
                    import requests
                    import os
                    import re
                    
                    serper_api_key = os.getenv("SERPER_API_KEY")
                    if not serper_api_key:
                        raise ValueError("æœªé…ç½®SERPER_API_KEY")
                        
                    # å¤„ç†è‚¡ç¥¨åç§°
                    clean_ticker = ticker.replace('.SH', '').replace('.SZ', '').replace('.SS', '').replace('.HK', '')
                    stock_name = market_info.get('name', '').replace('æ¸¯è‚¡', '').replace('Aè‚¡', '')
                    
                    if not stock_name or stock_name == ticker:
                        # å°è¯•è·å–ä¸­æ–‡åç§°
                        try:
                            if is_hk:
                                from tradingagents.dataflows.interface import get_hk_stock_info_unified
                                info = get_hk_stock_info_unified(ticker)
                            else:
                                from tradingagents.dataflows.interface import get_china_stock_info_unified
                                info = get_china_stock_info_unified(ticker)
                            
                            if isinstance(info, dict) and 'name' in info:
                                stock_name = info['name']
                        except:
                            pass
                    
                    # å†æ¬¡æ¸…ç†åç§° (é˜²æ­¢å¸¦æœ‰"æ¸¯è‚¡"å‰ç¼€)
                    if stock_name:
                        stock_name = stock_name.replace('æ¸¯è‚¡', '').replace('Aè‚¡', '')
                        # å»é™¤å¸¸è§çš„åç¼€å’Œå†—ä½™è¯ (å¦‚ï¼šäº¬ä¸œé›†å›¢-SW -> äº¬ä¸œ)
                        # 1. å…ˆè¿›è¡ŒNFKCæ ‡å‡†åŒ–ï¼Œå°†å…¨è§’å­—ç¬¦è½¬ä¸ºåŠè§’
                        import unicodedata
                        stock_name = unicodedata.normalize('NFKC', stock_name)
                        # 2. å»é™¤é›†å›¢ã€è‚¡ä»½ç­‰åç¼€ï¼Œä»¥åŠ -SW, -W ç­‰åç¼€
                        stock_name = re.sub(r'(é›†å›¢|è‚¡ä»½|æœ‰é™å…¬å¸|ï¼.*|-.*|\(.*\)|ï¼ˆ.*ï¼‰)', '', stock_name)
                        stock_name = stock_name.strip()
                    
                    # æ„é€  Google Dorks æŸ¥è¯¢ (é›ªçƒ + è‚¡å§)
                    base_query = f'"{clean_ticker}"'
                    if stock_name and stock_name != ticker:
                        base_query += f' "{stock_name}"'
                    
                    search_query = f'{base_query} èµ°åŠ¿ è§‚ç‚¹ site:xueqiu.com OR site:guba.eastmoney.com'
                    logger.info(f"ğŸ” [Serper] æœç´¢Query: {search_query}")
                    
                    url = "https://google.serper.dev/search"
                    headers = {
                        'X-API-KEY': serper_api_key,
                        'Content-Type': 'application/json'
                    }
                    
                    def perform_search(query):
                        payload = json.dumps({
                            "q": query,
                            "tbs": "qdr:d",  # è¿‡å»24å°æ—¶
                            "num": 20        # è·å–æ›´å¤šç»“æœ
                        })
                        response = requests.request("POST", url, headers=headers, data=payload)
                        return response.json().get('organic', [])

                    organic_results = perform_search(search_query)
                    
                    # å¦‚æœæ²¡æœ‰ç»“æœï¼Œå°è¯•æ”¾å®½æŸ¥è¯¢æ¡ä»¶ (å»æ‰"èµ°åŠ¿ è§‚ç‚¹")
                    if not organic_results:
                        logger.warning(f"âš ï¸ [Serper] æœªæœç´¢åˆ°ç›¸å…³è®¨è®ºï¼Œå°è¯•æ”¾å®½æŸ¥è¯¢æ¡ä»¶...")
                        search_query = f'{base_query} site:xueqiu.com OR site:guba.eastmoney.com'
                        logger.info(f"ğŸ” [Serper] æœç´¢Query (Fallback): {search_query}")
                        organic_results = perform_search(search_query)
                    
                    if organic_results:
                        discussions = []
                        for item in organic_results:
                            title = item.get('title', '')
                            snippet = item.get('snippet', '')
                            source = item.get('link', '')
                            
                            # è¯†åˆ«æ¥æºå¹³å°
                            platform = "æœªçŸ¥"
                            if "xueqiu.com" in source:
                                platform = "é›ªçƒ"
                            elif "eastmoney.com" in source:
                                platform = "è‚¡å§"
                                
                            discussions.append(f"- [{platform}] **{title}**: {snippet}")
                        
                        discussion_text = "\n".join(discussions)
                        
                        sentiment_summary = f"""
## ä¸­æ–‡å¸‚åœºæ•£æˆ·æƒ…ç»ªåˆ†æ (Serperæœç´¢ç»“æœ)

**è‚¡ç¥¨**: {ticker} ({stock_name})
**åˆ†ææ—¥æœŸ**: {curr_date} (æœ€è¿‘24å°æ—¶)

### æ•£æˆ·è®¨è®ºçƒ­ç‚¹
{discussion_text}

### æƒ…ç»ªç»¼è¿°
åŸºäºä¸Šè¿°æœç´¢ç»“æœï¼Œè¯·LLMè‡ªè¡Œæ€»ç»“æŠ•èµ„è€…æƒ…ç»ªï¼ˆä¹è§‚/æ‚²è§‚/ä¸­æ€§ï¼‰åŠä¸»è¦å…³æ³¨ç‚¹ã€‚
"""
                        result_data.append(sentiment_summary)
                        response_dict["content"] = sentiment_summary
                        response_dict["status"] = "success"
                        response_dict["source"] = "Serper/Google"
                        logger.info(f"âœ… [Serper] æˆåŠŸè·å– {len(organic_results)} æ¡æ•£æˆ·è®¨è®º")
                    else:
                        logger.warning(f"âš ï¸ [Serper] æœªæœç´¢åˆ°ç›¸å…³è®¨è®ºï¼Œè¿”å›é»˜è®¤ä¸­æ€§Dict")
                        response_dict["summary"] = "ã€ç³»ç»Ÿæç¤ºã€‘ç”±äºä¸­æ–‡ç¤¾äº¤åª’ä½“æ•°æ®æºæš‚æœªæ¥é€šï¼Œæš‚æ— å®æ—¶æƒ…ç»ªæ•°æ®ã€‚æ­¤ä¸ºå ä½ç»“æœï¼Œè¯·å‹¿åŸºäºæ­¤è¿›è¡Œäº¤æ˜“å†³ç­–ã€‚"
                        return response_dict

                except Exception as e:
                    logger.error(f"âŒ [Serper] æœç´¢å¤±è´¥: {e}")
                    response_dict["summary"] = f"ã€ç³»ç»Ÿæç¤ºã€‘æƒ…ç»ªåˆ†æå¤±è´¥: {str(e)}"
                    response_dict["status"] = "error"
                    response_dict["error"] = str(e)
                    return response_dict

            else:
                # ç¾è‚¡ï¼šä½¿ç”¨Redditæƒ…ç»ªåˆ†æ
                logger.info(f"ğŸ‡ºğŸ‡¸ [ç»Ÿä¸€æƒ…ç»ªå·¥å…·] å¤„ç†ç¾è‚¡æƒ…ç»ª...")

                try:
                    from tradingagents.dataflows.interface import get_reddit_sentiment

                    sentiment_data = get_reddit_sentiment(ticker, curr_date)
                    result_data.append(f"## ç¾è‚¡Redditæƒ…ç»ª\n{sentiment_data}")
                    response_dict["content"] = sentiment_data
                    response_dict["source"] = "Reddit"
                    response_dict["status"] = "success"
                except Exception as e:
                    result_data.append(f"## ç¾è‚¡Redditæƒ…ç»ª\nè·å–å¤±è´¥: {e}")
                    response_dict["content"] = f"## ç¾è‚¡Redditæƒ…ç»ª\nè·å–å¤±è´¥: {e}"
                    response_dict["status"] = "error"

            # ç»„åˆæ‰€æœ‰æ•°æ®
            combined_result = f"""# {ticker} æƒ…ç»ªåˆ†æ

**è‚¡ç¥¨ç±»å‹**: {market_info['market_name']}
**åˆ†ææ—¥æœŸ**: {curr_date}

{chr(10).join(result_data)}

---
*æ•°æ®æ¥æº: Serper (Google Search) / Reddit*
"""
            response_dict["content"] = combined_result
            
            logger.info(f"ğŸ˜Š [ç»Ÿä¸€æƒ…ç»ªå·¥å…·] æ•°æ®è·å–å®Œæˆï¼Œæ€»é•¿åº¦: {len(combined_result)}")
            return response_dict

        except Exception as e:
            error_msg = f"ç»Ÿä¸€æƒ…ç»ªåˆ†æå·¥å…·æ‰§è¡Œå¤±è´¥: {str(e)}"
            logger.error(f"âŒ [ç»Ÿä¸€æƒ…ç»ªå·¥å…·] {error_msg}")
            return {
                "status": "error",
                "error": str(e),
                "ticker": ticker,
                "content": error_msg
            }

    def _get_us_share_news(self, stock_code: str, max_news: int, model_info: str = "") -> str:
        """è·å–ç¾è‚¡æ–°é—»"""
        logger.info(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] è·å–ç¾è‚¡ {stock_code} æ–°é—»")
        
        # è·å–å½“å‰æ—¥æœŸ
        curr_date = datetime.now().strftime("%Y-%m-%d")
        
        # ä¼˜å…ˆçº§1: OpenAIå…¨çƒæ–°é—»
        try:
            if hasattr(self.toolkit, 'get_global_news_openai'):
                logger.info(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] å°è¯•OpenAIç¾è‚¡æ–°é—»...")
                # ä½¿ç”¨LangChainå·¥å…·çš„æ­£ç¡®è°ƒç”¨æ–¹å¼ï¼š.invoke()æ–¹æ³•å’Œå­—å…¸å‚æ•°
                result = self.toolkit.get_global_news_openai.invoke({"curr_date": curr_date})
                if result and len(result.strip()) > 50:
                    logger.info(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] âœ… OpenAIç¾è‚¡æ–°é—»è·å–æˆåŠŸ: {len(result)} å­—ç¬¦")
                    return self._format_news_result(result, "OpenAIç¾è‚¡æ–°é—»", model_info)
        except Exception as e:
            logger.warning(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] OpenAIç¾è‚¡æ–°é—»è·å–å¤±è´¥: {e}")
        
        # ä¼˜å…ˆçº§2: Googleæ–°é—»ï¼ˆè‹±æ–‡æœç´¢ï¼‰
        try:
            if hasattr(self.toolkit, 'get_google_news'):
                logger.info(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] å°è¯•Googleç¾è‚¡æ–°é—»...")
                query = f"{stock_code} stock news earnings financial"
                # ä½¿ç”¨LangChainå·¥å…·çš„æ­£ç¡®è°ƒç”¨æ–¹å¼ï¼š.invoke()æ–¹æ³•å’Œå­—å…¸å‚æ•°
                result = self.toolkit.get_google_news.invoke({"query": query, "curr_date": curr_date})
                if result and len(result.strip()) > 50:
                    logger.info(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] âœ… Googleç¾è‚¡æ–°é—»è·å–æˆåŠŸ: {len(result)} å­—ç¬¦")
                    return self._format_news_result(result, "Googleç¾è‚¡æ–°é—»", model_info)
        except Exception as e:
            logger.warning(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] Googleç¾è‚¡æ–°é—»è·å–å¤±è´¥: {e}")
        
        # ä¼˜å…ˆçº§3: FinnHubæ–°é—»ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        try:
            if hasattr(self.toolkit, 'get_finnhub_news'):
                logger.info(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] å°è¯•FinnHubç¾è‚¡æ–°é—»...")
                # ä½¿ç”¨LangChainå·¥å…·çš„æ­£ç¡®è°ƒç”¨æ–¹å¼ï¼š.invoke()æ–¹æ³•å’Œå­—å…¸å‚æ•°
                result = self.toolkit.get_finnhub_news.invoke({"symbol": stock_code, "max_results": min(max_news, 50)})
                if result and len(result.strip()) > 50:
                    logger.info(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] âœ… FinnHubç¾è‚¡æ–°é—»è·å–æˆåŠŸ: {len(result)} å­—ç¬¦")
                    return self._format_news_result(result, "FinnHubç¾è‚¡æ–°é—»", model_info)
        except Exception as e:
            logger.warning(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] FinnHubç¾è‚¡æ–°é—»è·å–å¤±è´¥: {e}")
        
        return "âŒ æ— æ³•è·å–ç¾è‚¡æ–°é—»æ•°æ®ï¼Œæ‰€æœ‰æ–°é—»æºå‡ä¸å¯ç”¨"
    
    def _format_news_result(self, news_content: str, source: str, model_info: str = "") -> str:
        """æ ¼å¼åŒ–æ–°é—»ç»“æœ"""
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        
        # ğŸ” æ·»åŠ è°ƒè¯•æ—¥å¿—ï¼šæ‰“å°åŸå§‹æ–°é—»å†…å®¹
        logger.info(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] ğŸ“‹ åŸå§‹æ–°é—»å†…å®¹é¢„è§ˆ (å‰500å­—ç¬¦): {news_content[:500]}")
        logger.info(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] ğŸ“Š åŸå§‹å†…å®¹é•¿åº¦: {len(news_content)} å­—ç¬¦")
        
        # æ£€æµ‹æ˜¯å¦ä¸ºGoogle/Geminiæ¨¡å‹
        is_google_model = any(keyword in model_info.lower() for keyword in ['google', 'gemini', 'gemma'])
        original_length = len(news_content)
        google_control_applied = False
        
        # ğŸ” æ·»åŠ Googleæ¨¡å‹æ£€æµ‹æ—¥å¿—
        if is_google_model:
            logger.info(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] ğŸ¤– æ£€æµ‹åˆ°Googleæ¨¡å‹ï¼Œå¯ç”¨ç‰¹æ®Šå¤„ç†")
        
        # å¯¹Googleæ¨¡å‹è¿›è¡Œç‰¹æ®Šçš„é•¿åº¦æ§åˆ¶
        if is_google_model and len(news_content) > 5000:  # é™ä½é˜ˆå€¼åˆ°5000å­—ç¬¦
            logger.warning(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] ğŸ”§ æ£€æµ‹åˆ°Googleæ¨¡å‹ï¼Œæ–°é—»å†…å®¹è¿‡é•¿({len(news_content)}å­—ç¬¦)ï¼Œè¿›è¡Œé•¿åº¦æ§åˆ¶...")
            
            # æ›´ä¸¥æ ¼çš„é•¿åº¦æ§åˆ¶ç­–ç•¥
            lines = news_content.split('\n')
            important_lines = []
            char_count = 0
            target_length = 3000  # ç›®æ ‡é•¿åº¦è®¾ä¸º3000å­—ç¬¦
            
            # ç¬¬ä¸€è½®ï¼šä¼˜å…ˆä¿ç•™åŒ…å«å…³é”®è¯çš„é‡è¦è¡Œ
            for line in lines:
                line = line.strip()
                if not line:
                    continue
                    
                # æ£€æŸ¥æ˜¯å¦åŒ…å«é‡è¦å…³é”®è¯
                important_keywords = ['è‚¡ç¥¨', 'å…¬å¸', 'è´¢æŠ¥', 'ä¸šç»©', 'æ¶¨è·Œ', 'ä»·æ ¼', 'å¸‚å€¼', 'è¥æ”¶', 'åˆ©æ¶¦', 
                                    'å¢é•¿', 'ä¸‹è·Œ', 'ä¸Šæ¶¨', 'ç›ˆåˆ©', 'äºæŸ', 'æŠ•èµ„', 'åˆ†æ', 'é¢„æœŸ', 'å…¬å‘Š']
                
                is_important = any(keyword in line for keyword in important_keywords)
                
                if is_important and char_count + len(line) < target_length:
                    important_lines.append(line)
                    char_count += len(line)
                elif not is_important and char_count + len(line) < target_length * 0.7:  # éé‡è¦å†…å®¹æ›´ä¸¥æ ¼é™åˆ¶
                    important_lines.append(line)
                    char_count += len(line)
                
                # å¦‚æœå·²è¾¾åˆ°ç›®æ ‡é•¿åº¦ï¼Œåœæ­¢æ·»åŠ 
                if char_count >= target_length:
                    break
            
            # å¦‚æœæå–çš„é‡è¦å†…å®¹ä»ç„¶è¿‡é•¿ï¼Œè¿›è¡Œè¿›ä¸€æ­¥æˆªæ–­
            if important_lines:
                processed_content = '\n'.join(important_lines)
                if len(processed_content) > target_length:
                    processed_content = processed_content[:target_length] + "...(å†…å®¹å·²æ™ºèƒ½æˆªæ–­)"
                
                news_content = processed_content
                google_control_applied = True
                logger.info(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] âœ… Googleæ¨¡å‹æ™ºèƒ½é•¿åº¦æ§åˆ¶å®Œæˆï¼Œä»{original_length}å­—ç¬¦å‹ç¼©è‡³{len(news_content)}å­—ç¬¦")
            else:
                # å¦‚æœæ²¡æœ‰é‡è¦è¡Œï¼Œç›´æ¥æˆªæ–­åˆ°ç›®æ ‡é•¿åº¦
                news_content = news_content[:target_length] + "...(å†…å®¹å·²å¼ºåˆ¶æˆªæ–­)"
                google_control_applied = True
                logger.info(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] âš ï¸ Googleæ¨¡å‹å¼ºåˆ¶æˆªæ–­è‡³{target_length}å­—ç¬¦")
        
        # è®¡ç®—æœ€ç»ˆçš„æ ¼å¼åŒ–ç»“æœé•¿åº¦ï¼Œç¡®ä¿æ€»é•¿åº¦åˆç†
        base_format_length = 300  # æ ¼å¼åŒ–æ¨¡æ¿çš„å¤§æ¦‚é•¿åº¦
        if is_google_model and (len(news_content) + base_format_length) > 4000:
            # å¦‚æœåŠ ä¸Šæ ¼å¼åŒ–åä»ç„¶è¿‡é•¿ï¼Œè¿›ä¸€æ­¥å‹ç¼©æ–°é—»å†…å®¹
            max_content_length = 3500
            if len(news_content) > max_content_length:
                news_content = news_content[:max_content_length] + "...(å·²ä¼˜åŒ–é•¿åº¦)"
                google_control_applied = True
                logger.info(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] ğŸ”§ Googleæ¨¡å‹æœ€ç»ˆé•¿åº¦ä¼˜åŒ–ï¼Œå†…å®¹é•¿åº¦: {len(news_content)}å­—ç¬¦")
        
        formatted_result = f"""
=== ğŸ“° æ–°é—»æ•°æ®æ¥æº: {source} ===
è·å–æ—¶é—´: {timestamp}
æ•°æ®é•¿åº¦: {len(news_content)} å­—ç¬¦
{f"æ¨¡å‹ç±»å‹: {model_info}" if model_info else ""}
{f"ğŸ”§ Googleæ¨¡å‹é•¿åº¦æ§åˆ¶å·²åº”ç”¨ (åŸé•¿åº¦: {original_length} å­—ç¬¦)" if google_control_applied else ""}

=== ğŸ“‹ æ–°é—»å†…å®¹ ===
{news_content}

=== âœ… æ•°æ®çŠ¶æ€ ===
çŠ¶æ€: æˆåŠŸè·å–
æ¥æº: {source}
æ—¶é—´æˆ³: {timestamp}
"""
        return formatted_result.strip()


def create_unified_news_tool(toolkit):
    """åˆ›å»ºç»Ÿä¸€æ–°é—»å·¥å…·å‡½æ•°"""
    analyzer = UnifiedNewsAnalyzer(toolkit)
    
    def get_stock_news_unified(stock_code: str, max_news: int = 100, model_info: str = ""):
        """
        ç»Ÿä¸€æ–°é—»è·å–å·¥å…·
        
        Args:
            stock_code (str): è‚¡ç¥¨ä»£ç  (æ”¯æŒAè‚¡å¦‚000001ã€æ¸¯è‚¡å¦‚0700.HKã€ç¾è‚¡å¦‚AAPL)
            max_news (int): æœ€å¤§æ–°é—»æ•°é‡ï¼Œé»˜è®¤100
            model_info (str): å½“å‰ä½¿ç”¨çš„æ¨¡å‹ä¿¡æ¯ï¼Œç”¨äºç‰¹æ®Šå¤„ç†
        
        Returns:
            str: æ ¼å¼åŒ–çš„æ–°é—»å†…å®¹
        """
        if not stock_code:
            return "âŒ é”™è¯¯: æœªæä¾›è‚¡ç¥¨ä»£ç "
        
        return analyzer.get_stock_news_unified(stock_code, max_news, model_info)
    
    # è®¾ç½®å·¥å…·å±æ€§
    get_stock_news_unified.name = "get_stock_news_unified"
    get_stock_news_unified.description = """
ç»Ÿä¸€æ–°é—»è·å–å·¥å…· - æ ¹æ®è‚¡ç¥¨ä»£ç è‡ªåŠ¨è·å–ç›¸åº”å¸‚åœºçš„æ–°é—»

åŠŸèƒ½:
- è‡ªåŠ¨è¯†åˆ«è‚¡ç¥¨ç±»å‹ï¼ˆAè‚¡/æ¸¯è‚¡/ç¾è‚¡ï¼‰
- æ ¹æ®è‚¡ç¥¨ç±»å‹é€‰æ‹©æœ€ä½³æ–°é—»æº
- Aè‚¡: ä¼˜å…ˆä¸œæ–¹è´¢å¯Œ -> Googleä¸­æ–‡ -> OpenAI
- æ¸¯è‚¡: ä¼˜å…ˆGoogle -> OpenAI -> å®æ—¶æ–°é—»
- ç¾è‚¡: ä¼˜å…ˆOpenAI -> Googleè‹±æ–‡ -> FinnHub
- è¿”å›æ ¼å¼åŒ–çš„æ–°é—»å†…å®¹
- æ”¯æŒGoogleæ¨¡å‹çš„ç‰¹æ®Šé•¿åº¦æ§åˆ¶
"""
    
    def get_stock_sentiment_unified(ticker: str, curr_date: str) -> str:
        """
        ç»Ÿä¸€çš„è‚¡ç¥¨æƒ…ç»ªåˆ†æå·¥å…·
        è‡ªåŠ¨è¯†åˆ«è‚¡ç¥¨ç±»å‹ï¼ˆAè‚¡ã€æ¸¯è‚¡ã€ç¾è‚¡ï¼‰å¹¶è°ƒç”¨ç›¸åº”çš„æƒ…ç»ªæ•°æ®æº
        
        Args:
            ticker: è‚¡ç¥¨ä»£ç 
            curr_date: å½“å‰æ—¥æœŸ
            
        Returns:
            str: æ ¼å¼åŒ–çš„æƒ…ç»ªåˆ†æå†…å®¹
        """
        if not ticker:
            return "âŒ é”™è¯¯: æœªæä¾›è‚¡ç¥¨ä»£ç "
            
        return analyzer.get_stock_sentiment_unified(ticker, curr_date)
    
    # è®¾ç½®å·¥å…·å±æ€§
    get_stock_sentiment_unified.name = "get_stock_sentiment_unified"
    get_stock_sentiment_unified.description = """
ç»Ÿä¸€è‚¡ç¥¨æƒ…ç»ªåˆ†æå·¥å…· - æ ¹æ®è‚¡ç¥¨ä»£ç è‡ªåŠ¨è·å–ç›¸åº”å¸‚åœºçš„æƒ…ç»ªæ•°æ®

åŠŸèƒ½:
- è‡ªåŠ¨è¯†åˆ«è‚¡ç¥¨ç±»å‹ï¼ˆAè‚¡/æ¸¯è‚¡/ç¾è‚¡ï¼‰
- æ¸¯è‚¡/Aè‚¡: ä½¿ç”¨Serperæœç´¢é›ªçƒ/è‚¡å§çš„æ•£æˆ·è®¨è®º
- ç¾è‚¡: ä½¿ç”¨Reddit/Twitteræƒ…ç»ªæ•°æ®
- è¿”å›æ ¼å¼åŒ–çš„æƒ…ç»ªåˆ†ææŠ¥å‘Š
"""

    return get_stock_news_unified, get_stock_sentiment_unified